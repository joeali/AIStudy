"""
AI Study Companion - åç«¯æœåŠ¡
æä¾› OCR è¯†åˆ«, é¢˜ç›®åˆ†æç­‰ API
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
import base64
import io
import json
import re
import requests
from PIL import Image
from pydantic import BaseModel
from typing import List, Optional
import numpy as np
import asyncio
import threading
import time
import sys
from queue import Queue
from functools import wraps

# ==================== å¯¼å…¥æ™ºèƒ½åˆ†ææ¨¡å— ====================
from smart_analysis import (
    analyze_content_type,
    generate_learning_analysis_prompt,
    generate_mistake_guide_prompt
)

# ==================== é…ç½® ====================
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡è¯»å–API Keyï¼ˆå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
GLM_API_KEY = os.getenv("GLM_API_KEY", "5f53890e74fa465a8ad1a95409db864c.roWm4OnFKpTIIdDJ")
GLM_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

# æ£€æŸ¥API Keyæ˜¯å¦ä¸ºé»˜è®¤å€¼
if GLM_API_KEY == "5f53890e74fa465a8ad1a95409db864c.roWm4OnFKpTIIdDJ":
    print("=" * 60)
    print("âš ï¸  è­¦å‘Š: ä½¿ç”¨é»˜è®¤çš„GLM API Key")
    print("å¦‚éœ€ä½¿ç”¨è‡ªå·±çš„API Keyï¼Œè¯·åˆ›å»º backend/.env æ–‡ä»¶:")
    print("  GLM_API_KEY=your_api_key_here")
    print("=" * 60)

# ==================== API è¯·æ±‚é˜Ÿåˆ— ====================
from concurrent.futures import ThreadPoolExecutor

# åˆ›å»ºè¯·æ±‚é˜Ÿåˆ—
request_queue = Queue()
# è¯·æ±‚å¤„ç†çº¿ç¨‹æ± (å¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚)
queue_executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix="glm_api_queue")
# è¯·æ±‚é”(ç¡®ä¿åŒæ—¶åªæœ‰1ä¸ªGLM APIè°ƒç”¨)
glm_api_lock = threading.Lock()
# è¯·æ±‚IDè®¡æ•°å™¨
request_counter = 0
request_counter_lock = threading.Lock()

def get_request_id():
    """ç”Ÿæˆå”¯ä¸€çš„è¯·æ±‚ID"""
    global request_counter
    with request_counter_lock:
        request_counter += 1
        return request_counter

# ==================== åˆ›å»º FastAPI åº”ç”¨ ====================
app = FastAPI(
    title="AI Study Companion API",
    description="AI å­¦ä¹ åŠ©æ‰‹åç«¯æœåŠ¡ - OCR è¯†åˆ«ä¸é¢˜ç›®åˆ†æ",
    version="1.0.0"
)

# ==================== CORS é…ç½® ====================
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æº(ç”Ÿäº§ç¯å¢ƒéœ€é™åˆ¶)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== æ•°æ®æ¨¡å‹ ====================
class OCRRequest(BaseModel):
    """OCR è¯·æ±‚æ¨¡å‹"""
    image_data: str  # base64 ç¼–ç çš„å›¾ç‰‡
    image_type: str = "image/jpeg"  # å›¾ç‰‡ç±»å‹

class QuestionAnalyzeRequest(BaseModel):
    """é¢˜ç›®åˆ†æè¯·æ±‚"""
    image_data: Optional[str] = None
    image_type: Optional[str] = "image/jpeg"
    question_text: Optional[str] = None
    student_answer: Optional[str] = None

class ChatRequest(BaseModel):
    """å¯¹è¯è¯·æ±‚"""
    message: str
    conversation_history: Optional[List[dict]] = []
    image_data: Optional[str] = None

class DiagnoseRequest(BaseModel):
    """è¯Šæ–­è¯·æ±‚"""
    question: str  # é¢˜ç›®å†…å®¹
    student_answer: str  # å­¦ç”Ÿçš„é”™è¯¯ç­”æ¡ˆ
    image_data: Optional[str] = None  # å¯é€‰çš„é¢˜ç›®å›¾ç‰‡

class GuideRequest(BaseModel):
    """å¼•å¯¼è¯·æ±‚"""
    question: str  # é¢˜ç›®å†…å®¹
    diagnosis: str  # è¯Šæ–­ç»“æœ
    student_response: Optional[str] = None  # å­¦ç”Ÿçš„å›ç­”(ç¬¬ä¸€è½®ä¸ºç©º)
    conversation_history: Optional[List[dict]] = []  # å¯¹è¯å†å²

class DetectMistakesRequest(BaseModel):
    """é”™é¢˜æ£€æµ‹è¯·æ±‚"""
    image_data: str  # base64 ç¼–ç çš„å›¾ç‰‡
    image_type: str = "image/jpeg"  # å›¾ç‰‡ç±»å‹
    user_marks: Optional[List[dict]] = []  # ç”¨æˆ·æ‰‹åŠ¨æ ‡è®°çš„é”™é¢˜ä½ç½® [{"x": 50, "y": 30}, ...]

# ==================== å·¥å…·å‡½æ•° ====================
def decode_base64_image(base64_str: str) -> Image.Image:
    """è§£ç  base64 å›¾ç‰‡"""
    # ç§»é™¤ data:image/xxx;base64, å‰ç¼€
    if "," in base64_str:
        base64_str = base64_str.split(",")[1]

    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image

def encode_image_to_base64(image: Image.Image, quality: int = 85) -> str:
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG", quality=quality)
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return img_str

def call_glm_api(messages: list, model: str = "glm-4v", max_retries: int = 3, skip_delay: bool = False, max_tokens: int = 2000) -> str:
    """è°ƒç”¨ GLM API(å¸¦æ’é˜Ÿå’Œé‡è¯•æœºåˆ¶)

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹åç§°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        skip_delay: æ˜¯å¦è·³è¿‡è¯·æ±‚å»¶è¿Ÿ(ç”¨äºå¿«é€Ÿå“åº”åœºæ™¯)
        max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°(ç”¨äºæ§åˆ¶å“åº”é€Ÿåº¦)
    """
    import time

    req_id = get_request_id()
    print(f"[API #{req_id}] ç­‰å¾…è·å–GLM APIé”...")

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–é”(ç¡®ä¿é”ä¸€å®šä¼šè¢«é‡Šæ”¾)
    with glm_api_lock:
        print(f"[API #{req_id}] å·²è·å–é”,å¼€å§‹è°ƒç”¨")

        headers = {
            "Authorization": f"Bearer {GLM_API_KEY}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": max_tokens
        }

        # æ·»åŠ æœ€å°è¯·æ±‚é—´éš”(é¿å…è¿‡äºé¢‘ç¹),å¿«é€Ÿæ¨¡å¼è·³è¿‡
        if not skip_delay:
            time.sleep(1)

        for attempt in range(max_retries):
            try:
                print(f"[API #{req_id}] å‘é€è¯·æ±‚åˆ°GLM... (å°è¯• {attempt + 1}/{max_retries})")
                response = requests.post(
                    GLM_API_URL,
                    headers=headers,
                    json=payload,
                    timeout=60
                )

                # å¤„ç† 429 å¹¶å‘é™åˆ¶é”™è¯¯
                if response.status_code == 429:
                    error_detail = response.json() if response.content else {}
                    error_msg = error_detail.get('error', {}).get('message', 'å¹¶å‘è¯·æ±‚è¿‡å¤š')

                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä½™é¢ä¸è¶³
                    if 'ä½™é¢' in error_msg or 'å……å€¼' in error_msg or 'èµ„æºåŒ…' in error_msg:
                        print(f"[API #{req_id}] âŒ APIä½™é¢ä¸è¶³")
                        raise HTTPException(
                            status_code=429,
                            detail=f"âš ï¸ APIä½™é¢ä¸è¶³\n\næ‚¨çš„GLM APIè´¦æˆ·ä½™é¢å·²ç”¨å®Œï¼Œè¯·å……å€¼åå†ä½¿ç”¨ã€‚\n\nğŸ“ è§£å†³æ–¹æ³•:\n1. è®¿é—® https://open.bigmodel.cn/ å……å€¼\n2. æˆ–åœ¨ backend/.env æ–‡ä»¶ä¸­é…ç½®å…¶ä»–API Key\n3. æ–°ç”¨æˆ·é€šå¸¸æœ‰å…è´¹é¢åº¦ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°"
                        )

                    if attempt < max_retries - 1:
                        # æŒ‡æ•°é€€é¿: 3ç§’, 6ç§’, 12ç§’
                        wait_time = [3, 6, 12][attempt]
                        print(f"[API #{req_id}] âš ï¸ é‡åˆ°å¹¶å‘é™åˆ¶,ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise HTTPException(
                            status_code=429,
                            detail=f"GLM API å¹¶å‘é™åˆ¶: {error_msg}. è¯·ç¨åé‡è¯•. "
                        )

                if response.status_code != 200:
                    error_detail = response.json() if response.content else {}
                    error_msg = error_detail.get('error', {}).get('message', response.text)
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f"GLM API é”™è¯¯: {error_msg}"
                    )

                result = response.json()
                print(f"[API #{req_id}] âœ… è¯·æ±‚æˆåŠŸ")
                print(f"[API #{req_id}] å“åº”ç»“æ„: {list(result.keys()) if isinstance(result, dict) else type(result)}")

                # æ£€æŸ¥choices
                if 'choices' not in result:
                    print(f"[API #{req_id}] âŒ å“åº”ä¸­æ²¡æœ‰choices")
                    print(f"[API #{req_id}] å®Œæ•´å“åº”: {json.dumps(result, ensure_ascii=False)[:500]}")
                    sys.stdout.flush()
                    raise HTTPException(
                        status_code=500,
                        detail="GLM API è¿”å›æ ¼å¼å¼‚å¸¸: ç¼ºå°‘choiceså­—æ®µ"
                    )

                if len(result['choices']) == 0:
                    print(f"[API #{req_id}] âŒ choicesä¸ºç©º")
                    sys.stdout.flush()
                    raise HTTPException(
                        status_code=500,
                        detail="GLM API è¿”å›ç©ºç»“æœ"
                    )

                print(f"[API #{req_id}] choices[0] keys: {list(result['choices'][0].keys())}")

                if 'message' not in result['choices'][0]:
                    print(f"[API #{req_id}] âŒ choices[0]ä¸­æ²¡æœ‰messageå­—æ®µ")
                    print(f"[API #{req_id}] choices[0]: {result['choices'][0]}")
                    sys.stdout.flush()
                    raise HTTPException(
                        status_code=500,
                        detail="GLM API è¿”å›æ ¼å¼å¼‚å¸¸: ç¼ºå°‘messageå­—æ®µ"
                    )

                print(f"[API #{req_id}] message keys: {list(result['choices'][0]['message'].keys())}")
                content = result['choices'][0]['message'].get('content', '')
                print(f"[API #{req_id}] å†…å®¹ç±»å‹: {type(content)}")
                print(f"[API #{req_id}] å†…å®¹é•¿åº¦: {len(content) if content else 0}")

                # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
                if not content or not content.strip():
                    print(f"[API #{req_id}] âŒ APIè¿”å›å†…å®¹ä¸ºç©º")
                    sys.stdout.flush()
                    raise HTTPException(
                        status_code=500,
                        detail="GLM API è¿”å›å†…å®¹ä¸ºç©º,è¯·é‡è¯•"
                    )

                print(f"[API #{req_id}] å†…å®¹repr: {repr(content[:100])}")
                print(f"[API #{req_id}] å†…å®¹é¢„è§ˆ: {content[:200]}")
                sys.stdout.flush()
                return content

            except requests.exceptions.Timeout as e:
                if attempt < max_retries - 1:
                    wait_time = [2, 4, 6][attempt]
                    print(f"[API #{req_id}] âš ï¸ è¯·æ±‚è¶…æ—¶,ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    raise HTTPException(status_code=504, detail=f"API è¯·æ±‚è¶…æ—¶: {str(e)}")

            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = [2, 4, 6][attempt]
                    print(f"[API #{req_id}] âš ï¸ ç½‘ç»œé”™è¯¯,ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    raise HTTPException(status_code=500, detail=f"API è°ƒç”¨å¤±è´¥: {str(e)}")

        raise HTTPException(status_code=500, detail="API è°ƒç”¨å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")


def parse_mistakes_from_response(response_text: str) -> dict:
    """ä»AIå“åº”ä¸­è§£æé”™é¢˜æ•°æ®

    æ”¯æŒå¤šç§æ ¼å¼:
    1. JSONä»£ç å—æ ¼å¼
    2. çº¯JSONæ ¼å¼
    3. Markdownåˆ—è¡¨æ ¼å¼
    """
    import re
    import json

    # æ–¹å¼1: å°è¯•è§£æJSONä»£ç å—
    json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except:
            pass

    # æ–¹å¼2: å°è¯•è§£æçº¯JSON
    try:
        return json.loads(response_text.strip())
    except:
        pass

    # æ–¹å¼3: å°è¯•æå–JSONå¯¹è±¡(æ²¡æœ‰ä»£ç å—)
    try:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        if start >= 0 and end > start:
            json_str = response_text[start:end]
            return json.loads(json_str)
    except:
        pass

    # æ–¹å¼4: è§£æMarkdownåˆ—è¡¨æ ¼å¼
    try:
        lines = response_text.split('\n')
        mistakes_list = []
        for line in lines:
            line = line.strip()
            if line.startswith('-') or line.startswith('â€¢'):
                match = re.search(r'ç¬¬(\d+)é¢˜', line)
                if match:
                    question_no = match.group(1)
                    mistakes_list.append({
                        "question_no": question_no,
                        "reason": "é”™é¢˜"
                    })
        if mistakes_list:
            return {
                "mistakes": mistakes_list,
                "summary": f"å…±æ‰¾åˆ°{len(mistakes_list)}é“é”™é¢˜"
            }
    except:
        pass

    return None


# ==================== API è·¯ç”± ====================

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "AI Study Companion API",
        "version": "1.0.0",
        "endpoints": {
            "/api/ocr/exam": "è¯•å· OCR è¯†åˆ«",
            "/api/analyze/question": "é¢˜ç›®åˆ†æ",
            "/api/chat": "AI å¯¹è¯",
            "/api/chat/stream": "AI å¯¹è¯(æµå¼)",
            "/api/diagnose/analyze": "è§£é¢˜è¯Šæ–­åˆ†æ",
            "/api/diagnose/analyze/stream": "è§£é¢˜è¯Šæ–­åˆ†æ(æµå¼)",
            "/api/diagnose/guide": "è‹æ ¼æ‹‰åº•å¼å¼•å¯¼",
            "/api/diagnose/guide/stream": "è‹æ ¼æ‹‰åº•å¼å¼•å¯¼(æµå¼)",
            "/api/detect/mistakes": "æ™ºèƒ½æ‰¾é”™é¢˜",
            "/api/detect/mistakes/stream": "æ™ºèƒ½æ‰¾é”™é¢˜(æµå¼)"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

@app.post("/api/ocr/exam")
async def ocr_exam_paper(request: OCRRequest):
    """
    è¯•å· OCR è¯†åˆ«

    ä½¿ç”¨ GLM-4V è¯†åˆ«è¯•å·ä¸­çš„é¢˜ç›®å’Œç­”æ¡ˆ
    """
    try:
        # è§£ç å›¾ç‰‡
        image = decode_base64_image(request.image_data)

        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸
        if image.width < 100 or image.height < 100:
            raise HTTPException(
                status_code=400,
                detail=f"å›¾ç‰‡å°ºå¯¸å¤ªå° ({image.width}x{image.height}),è¯·ä¸Šä¼ æ›´æ¸…æ™°çš„å›¾ç‰‡"
            )

        # å‹ç¼©å›¾ç‰‡ä»¥åŠ å¿«ä¼ è¾“
        base64_image = encode_image_to_base64(image, quality=85)

        # æ„å»º prompt(ç®€åŒ–ç‰ˆ,æ›´å®¹æ˜“è§£æ)
        prompt = """è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰é¢˜ç›®å†…å®¹. 

è¯·ä»¥JSONæ ¼å¼è¿”å›: 
```json
{
  "questions": [
    {
      "question_no": "é¢˜å·",
      "question_text": "é¢˜ç›®å†…å®¹",
      "student_answer": "å­¦ç”Ÿç­”æ¡ˆ"
    }
  ]
}
```"""

        # è°ƒç”¨ GLM-4V API
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    },
                    {
                        "type": "text",
                        "text": prompt
                    }
                ]
            }
        ]

        response_text = call_glm_api(messages, model="glm-4v")

        # è§£æå“åº”(å¤šç§æ–¹å¼å°è¯•)
        data = None

        # æ–¹å¼1: å°è¯•æå– JSON ä»£ç å—
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(1))
            except:
                pass

        # æ–¹å¼2: å°è¯•æå–èŠ±æ‹¬å·å†…çš„ JSON
        if not data:
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                try:
                    data = json.loads(json_match.group(0))
                except:
                    pass

        # å¦‚æœ JSON è§£æå¤±è´¥,è¿”å›åŸå§‹æ–‡æœ¬
        if not data:
            return {
                "success": True,
                "data": {
                    "questions": [],
                    "note": "æ— æ³•è§£æ JSON,ä»¥ä¸‹æ˜¯åŸå§‹è¯†åˆ«ç»“æœ"
                },
                "raw_response": response_text,
                "parsed": False
            }

        return {
            "success": True,
            "data": data,
            "raw_response": response_text,
            "parsed": True
        }

    except HTTPException:
        raise
    except json.JSONDecodeError as e:
        # è¿”å›éƒ¨åˆ†ç»“æœè€Œä¸æ˜¯æŠ¥é”™
        return {
            "success": True,
            "data": {"questions": []},
            "error": f"JSON è§£æå¤±è´¥: {str(e)}",
            "parsed": False
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"OCR è¯†åˆ«å¤±è´¥: {str(e)}")

@app.post("/api/analyze/question")
async def analyze_question(request: QuestionAnalyzeRequest):
    """
    é¢˜ç›®åˆ†æ

    åˆ†æé¢˜ç›®å†…å®¹,æå–é¢˜ç›®ä¿¡æ¯, åˆ¤æ–­ç­”æ¡ˆæ­£è¯¯
    """
    try:
        # æ„å»ºæ¶ˆæ¯
        content = []

        # å¦‚æœæœ‰å›¾ç‰‡
        if request.image_data:
            image = decode_base64_image(request.image_data)
            base64_image = encode_image_to_base64(image, quality=85)

            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            })

        # æ„å»º prompt
        prompt = """è¯·åˆ†æè¿™é“é¢˜ç›®ï¼Œå‘Šè¯‰æˆ‘ï¼š

1. é¢˜ç›®å†…å®¹
2. å­¦ç§‘
3. çŸ¥è¯†ç‚¹
4. æ­£ç¡®ç­”æ¡ˆï¼ˆå¦‚æœå¯ä»¥çœ‹å‡ºçš„è¯ï¼‰
5. å­¦ç”Ÿçš„ç­”æ¡ˆï¼ˆå¦‚æœè¯•å·ä¸Šæœ‰ï¼‰

è¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ã€‚"""

        content.append({
            "type": "text",
            "text": prompt
        })

        messages = [{
            "role": "user",
            "content": content
        }]

        response_text = call_glm_api(messages, model="glm-4v")

        # è¿”å›è‡ªç„¶è¯­è¨€åˆ†æç»“æœ
        return {
            "success": True,
            "data": {
                "analysis": response_text
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"é¢˜ç›®åˆ†æé”™è¯¯: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"é¢˜ç›®åˆ†æå¤±è´¥: {str(e)}")

@app.post("/api/chat")
async def chat(request: ChatRequest):
    """
    AI å¯¹è¯

    æä¾›å¯å‘å¼æ•™å­¦å¯¹è¯
    """
    try:
        # éªŒè¯è¾“å…¥
        if not request.message or not request.message.strip():
            return {
                "success": False,
                "error": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º",
                "response": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"
            }

        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []

        # è½¬æ¢å†å²æ¶ˆæ¯(é™åˆ¶é•¿åº¦é¿å…è¶…å‡º token é™åˆ¶)
        max_history = 10  # æœ€å¤šä¿ç•™æœ€è¿‘ 10 æ¡å†å²
        history = request.conversation_history[-max_history:] if request.conversation_history else []

        for msg in history:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            # è·³è¿‡ç©ºæ¶ˆæ¯
            if content and content.strip():
                # å¦‚æœæ¶ˆæ¯åŒ…å«å›¾ç‰‡,è·³è¿‡(ç®€åŒ–å¤„ç†)
                if "image" in str(msg).lower():
                    continue
                messages.append({
                    "role": role,
                    "content": content[:1000]  # é™åˆ¶æ¯æ¡æ¶ˆæ¯é•¿åº¦
                })

        # æ·»åŠ å½“å‰æ¶ˆæ¯
        if request.image_data:
            try:
                # å¦‚æœæœ‰å›¾ç‰‡,ä½¿ç”¨å¤šæ¨¡æ€
                image = decode_base64_image(request.image_data)

                # æ£€æŸ¥å›¾ç‰‡å¤§å°
                if image.width * image.height > 4000000:  # é™åˆ¶å›¾ç‰‡å¤§å°
                    # ç¼©å°å›¾ç‰‡
                    ratio = min(800 / image.width, 600 / image.height)
                    new_width = int(image.width * ratio)
                    new_height = int(image.height * ratio)
                    image = image.resize((new_width, new_height))

                base64_image = encode_image_to_base64(image, quality=75)

                # åˆ¤æ–­æ˜¯å¦éœ€è¦å¯åŠ¨è¯Šæ–­æµç¨‹
                user_message = request.message or "è¯·å¸®æˆ‘çœ‹çœ‹è¿™é“é¢˜"
                needs_diagnosis = any(keyword in user_message for keyword in
                    ['ä¸ä¼š', 'é”™äº†', 'é”™è¯¯', 'ä¸æ‡‚', 'ä¸ä¼šåš', 'åšé”™äº†', 'è®²è§£', 'æ€ä¹ˆåš', 'å¸®æˆ‘', 'è¯·'])

                if needs_diagnosis:
                    # ä½¿ç”¨è¯Šæ–­æ¡†æ¶
                    print(f"[è¯Šæ–­] æ£€æµ‹åˆ°é”™é¢˜è¯·æ±‚,å¯åŠ¨è¯Šæ–­æµç¨‹...")

                    try:
                        # ç¬¬ä¸€æ­¥: æå–é¢˜ç›®å†…å®¹
                        extract_prompt = """è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„é¢˜ç›®å†…å®¹,ä»¥ç®€æ´çš„æ ¼å¼è¿”å›é¢˜ç›®(ä¸è¦åŒ…å«è§£ç­”è¿‡ç¨‹). """

                        messages_with_image = [{
                            "role": "user",
                            "content": [
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{base64_image}"
                                    }
                                },
                                {
                                    "type": "text",
                                    "text": extract_prompt
                                }
                            ]
                        }]

                        question_text = call_glm_api(messages_with_image, model="glm-4v")
                        print(f"[è¯Šæ–­] é¢˜ç›®æå–æˆåŠŸ: {question_text[:50]}...")

                        # ç¬¬äºŒæ­¥: è¯Šæ–­é”™è¯¯
                        diagnose_prompt = f"""ä½ æ˜¯ä¸€ä½æœ‰20å¹´æ•™å­¦ç»éªŒçš„åˆä¸­æ•°å­¦è€å¸ˆ. 
å­¦ç”Ÿåšé”™äº†è¿™é“é¢˜: {question_text}
å­¦ç”Ÿè¯´: {user_message}

è¯·åˆ†æ: 
1. è¿™é“é¢˜è€ƒæŸ¥çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹æ˜¯ä»€ä¹ˆ
2. å­¦ç”Ÿæœ€å¯èƒ½åœ¨å“ªä¸ªç¯èŠ‚å‡ºé”™(æ¦‚å¿µä¸æ¸…/æ–¹æ³•ä¸å¯¹/è®¡ç®—å¤±è¯¯)
3. ç”¨ä¸€å¥è¯å‘Šè¯‰å­¦ç”Ÿä»–çš„é—®é¢˜åœ¨å“ªé‡Œ(è¦å…·ä½“,ä¸è¦æ³›æ³›è€Œè°ˆ)

è¯·ä»¥JSONæ ¼å¼è¿”å›: 
```json
{{
  "knowledge_point": "æ ¸å¿ƒçŸ¥è¯†ç‚¹",
  "error_type": "æ¦‚å¿µä¸æ¸…/æ–¹æ³•ä¸å¯¹/è®¡ç®—å¤±è¯¯",
  "problem_description": "ä¸€å¥è¯æè¿°å­¦ç”Ÿçš„é—®é¢˜",
  "analysis": "è¯¦ç»†åˆ†æ"
}}
```
åªè¿”å›JSON,ä¸è¦å…¶ä»–å†…å®¹. """

                        messages_diagnose = [{
                            "role": "user",
                            "content": diagnose_prompt
                        }]

                        diagnosis_result = call_glm_api(messages_diagnose, model="glm-4-flash")
                        print(f"[è¯Šæ–­] è¯Šæ–­å®Œæˆ")

                        # è§£æè¯Šæ–­ç»“æœ
                        json_match = re.search(r'\{[\s\S]*\}', diagnosis_result)
                        if json_match:
                            diagnosis_data = json.loads(json_match.group(0))

                            # ç¬¬ä¸‰æ­¥: ç”Ÿæˆå¼•å¯¼æ€§é—®é¢˜
                            guide_prompt = f"""ä½ æ˜¯ä¸€ä½è€å¿ƒçš„æ•°å­¦è€å¸ˆ,æ­£åœ¨ä¸€å¯¹ä¸€è¾…å¯¼å­¦ç”Ÿ. 

é¢˜ç›®: {question_text}
è¯Šæ–­ç»“æœ: {diagnosis_data.get('problem_description', '')}

è¯·ç”¨è‹æ ¼æ‹‰åº•å¼æé—®,ä¸€æ­¥æ­¥å¼•å¯¼å­¦ç”Ÿè‡ªå·±åšå‡ºæ¥. 
è§„åˆ™: 
- æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜
- ä¸è¦ç›´æ¥è¯´ç­”æ¡ˆ
- å¼•å¯¼è¦ä»æœ€åŸºæœ¬çš„è§‚å¯Ÿå¼€å§‹

ç°åœ¨è¯·å¼€å§‹å¼•å¯¼,æå‡ºç¬¬ä¸€ä¸ªé—®é¢˜æ¥å¯å‘å­¦ç”Ÿæ€è€ƒ(ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€). """

                            messages_guide = [{
                                "role": "user",
                                "content": guide_prompt
                            }]

                            guide_response = call_glm_api(messages_guide, model="glm-4-flash")
                            print(f"[è¯Šæ–­] å¼•å¯¼é—®é¢˜ç”Ÿæˆå®Œæˆ")

                            # è¿”å›è¯Šæ–­+å¼•å¯¼çš„ç»“æœ
                            return {
                                "success": True,
                                "response": f"""ğŸ“‹ **é¢˜ç›®åˆ†æ**
{question_text}

---

ğŸ“‹ **è¯Šæ–­ç»“æœ**
**çŸ¥è¯†ç‚¹**: {diagnosis_data.get('knowledge_point', 'æœªè¯†åˆ«')}
**é—®é¢˜**: {diagnosis_data.get('problem_description', 'æœªè¯†åˆ«')}

---

ğŸ‘¨â€ğŸ« **å¼€å§‹å¼•å¯¼**
{guide_response}

---
ğŸ’¡ è¯·å›ç­”è€å¸ˆçš„é—®é¢˜,æˆ‘ä¼šä¸€æ­¥æ­¥å¼•å¯¼ä½ æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ. (è¾“å…¥"é€€å‡º"è¿”å›æ™®é€šæ¨¡å¼)""",
                                "diagnosis": diagnosis_data,
                                "question": question_text,
                                "mode": "guidance"
                            }

                    except Exception as e:
                        print(f"[è¯Šæ–­] å¤±è´¥: {str(e)},é™çº§ä¸ºæ™®é€šå›ç­”")
                        # è¯Šæ–­å¤±è´¥,ç»§ç»­ä½¿ç”¨æ™®é€šæµç¨‹

                # æ™®é€šå›ç­”æµç¨‹ - ä½¿ç”¨è‹æ ¼æ‹‰åº•å¼•å¯¼å¼
                enhanced_prompt = f"""ä½ æ˜¯ä¸€ä½è€å¿ƒçš„è€å¸ˆ,æ­£åœ¨è¾…å¯¼å­¦ç”Ÿã€‚å­¦ç”Ÿé—®: {user_message}

è¯·ç”¨è‹æ ¼æ‹‰åº•å¼å¼•å¯¼æ–¹æ³•å¸®åŠ©å­¦ç”Ÿ:
1. **ä¸è¦ç›´æ¥ç»™å‡ºç­”æ¡ˆæˆ–è¯¦ç»†è§£é¢˜æ­¥éª¤**
2. æå‡ºå¯å‘æ€§çš„é—®é¢˜,å¼•å¯¼å­¦ç”Ÿè‡ªå·±æ€è€ƒ
3. æ¯æ¬¡åªé—®ä¸€ä¸ªå…³é”®é—®é¢˜
4. å¦‚æœå­¦ç”Ÿéœ€è¦å¸®åŠ©,ç»™å‡ºé€æ­¥é€’è¿›çš„æç¤º(å…ˆæµ…æç¤º,å†æ·±æç¤º)

å¼•å¯¼ç­–ç•¥:
- ç¬¬ä¸€æ­¥: æç¤ºå­¦ç”Ÿå›é¡¾ç›¸å…³çŸ¥è¯†ç‚¹
- ç¬¬äºŒæ­¥: å¼•å¯¼å­¦ç”Ÿåˆ†æé¢˜ç›®æ¡ä»¶
- ç¬¬ä¸‰æ­¥: æç¤ºè§£é¢˜æ€è·¯æ–¹å‘
- ä¸è¦ç›´æ¥å‘Šè¯‰å­¦ç”Ÿåº”è¯¥æ€ä¹ˆåš,è€Œæ˜¯é—®é—®é¢˜è®©ä»–ä»¬è‡ªå·±æ‰¾åˆ°æ–¹æ³•

ç°åœ¨è¯·æå‡ºç¬¬ä¸€ä¸ªå¼•å¯¼é—®é¢˜æ¥å¯å‘å­¦ç”Ÿæ€è€ƒã€‚"""

                messages.append({
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        },
                        {
                            "type": "text",
                            "text": enhanced_prompt
                        }
                    ]
                })
            except Exception as img_error:
                # å›¾ç‰‡å¤„ç†å¤±è´¥,é™çº§ä¸ºçº¯æ–‡æœ¬
                print(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(img_error)},ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å¼")
                messages.append({
                    "role": "user",
                    "content": request.message[:500]
                })
        else:
            # çº¯æ–‡æœ¬é—®é¢˜ä¹Ÿä½¿ç”¨å¼•å¯¼å¼
            enhanced_text_prompt = f"""ä½ æ˜¯ä¸€ä½è€å¿ƒçš„è€å¸ˆã€‚å­¦ç”Ÿé—®: {request.message}

è¯·ç”¨è‹æ ¼æ‹‰åº•å¼å¼•å¯¼æ–¹æ³•å¸®åŠ©å­¦ç”Ÿ:
- ä¸è¦ç›´æ¥ç»™ç­”æ¡ˆæˆ–è§£é¢˜æ­¥éª¤
- æå‡ºå¯å‘æ€§é—®é¢˜å¼•å¯¼å­¦ç”Ÿæ€è€ƒ
- æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜
- ç»™å‡ºé€’è¿›å¼æç¤º

è¯·æå‡ºç¬¬ä¸€ä¸ªå¼•å¯¼é—®é¢˜ã€‚"""

            messages.append({
                "role": "user",
                "content": enhanced_text_prompt[:1000]
            })

        # è°ƒç”¨ GLM API(call_glm_api å†…éƒ¨å·²å¤„ç†é‡è¯•)
        # æ ¹æ®æ˜¯å¦æœ‰å›¾ç‰‡é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        model = "glm-4v" if request.image_data else "glm-4-flash"
        try:
            response_text = call_glm_api(messages, model=model)
        except HTTPException as e:
            # å¤„ç† HTTP å¼‚å¸¸(åŒ…æ‹¬ 429 å¹¶å‘é™åˆ¶)
            return {
                "success": False,
                "error": e.detail,
                "response": f"æŠ±æ­‰,{e.detail}"
            }
        except Exception as e:
            # å¤„ç†å…¶ä»–å¼‚å¸¸
            return {
                "success": False,
                "error": str(e),
                "response": "æŠ±æ­‰,å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯,è¯·ç¨åé‡è¯•"
            }

        return {
            "success": True,
            "response": response_text[:2000]  # é™åˆ¶å“åº”é•¿åº¦
        }

    except HTTPException:
        raise
    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯
        import traceback
        print(f"å¯¹è¯ API é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")

        return {
            "success": False,
            "error": str(e),
            "response": "æŠ±æ­‰,å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜,è¯·é‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜"
        }

@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    AI å¯¹è¯(æµå¼è¾“å‡º)

    æä¾›å¯å‘å¼æ•™å­¦å¯¹è¯,ä½¿ç”¨ Server-Sent Events é€æ­¥è¿”å›å“åº”
    """
    async def generate_stream():
        try:
            # ç«‹å³å‘é€å¼€å§‹çŠ¶æ€
            yield f"data: {json.dumps({'status': 'starting', 'message': 'å¼€å§‹åˆ†æ...'})}\n\n"

            # éªŒè¯è¾“å…¥
            if not request.message or not request.message.strip():
                yield f"data: {json.dumps({'error': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º', 'done': True})}\n\n"
                return

            # å‘é€åˆ†æä¸­çŠ¶æ€
            yield f"data: {json.dumps({'status': 'analyzing', 'message': 'AIæ­£åœ¨åˆ†æä¸­...'})}\n\n"

            # æ„å»ºæ¶ˆæ¯å†å²
            messages = []

            # è½¬æ¢å†å²æ¶ˆæ¯
            max_history = 10
            history = request.conversation_history[-max_history:] if request.conversation_history else []

            for msg in history:
                role = msg.get("role", "user")
                content = msg.get("content", "")

                if content and content.strip():
                    if "image" in str(msg).lower():
                        continue
                    messages.append({
                        "role": role,
                        "content": content[:1000]
                    })

            # æ·»åŠ å½“å‰æ¶ˆæ¯
            if request.image_data:
                try:
                    image = decode_base64_image(request.image_data)

                    if image.width * image.height > 4000000:
                        ratio = min(800 / image.width, 600 / image.height)
                        new_width = int(image.width * ratio)
                        new_height = int(image.height * ratio)
                        image = image.resize((new_width, new_height))

                    base64_image = encode_image_to_base64(image, quality=75)

                    user_message = request.message or "è¯·å¸®æˆ‘çœ‹çœ‹è¿™é“é¢˜"
                    enhanced_prompt = f"""{user_message}

è¯·åˆ†æè¿™é“é¢˜ç›®,å¦‚æœæ˜¯è®¡ç®—é¢˜è¯·ç»™å‡ºè¯¦ç»†æ­¥éª¤,å¦‚æœæ˜¯åº”ç”¨é¢˜è¯·è¯´æ˜è§£é¢˜æ€è·¯. """

                    messages.append({
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            },
                            {
                                "type": "text",
                                "text": enhanced_prompt
                            }
                        ]
                    })
                except Exception as img_error:
                    print(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(img_error)}")
                    messages.append({
                        "role": "user",
                        "content": request.message[:500]
                    })
            else:
                messages.append({
                    "role": "user",
                    "content": request.message[:500]
                })

            # è°ƒç”¨ GLM API è·å–å®Œæ•´å“åº”
            model = "glm-4v" if request.image_data else "glm-4-flash"

            try:
                print("[æµå¼å¯¹è¯] å‘é€åˆ†æä¸­çŠ¶æ€...")
                # ç«‹å³å‘é€"åˆ†æä¸­"çŠ¶æ€
                yield f"data: {json.dumps({'status': 'analyzing', 'message': 'AIæ­£åœ¨åˆ†æä¸­...'})}\n\n"
                print(f"[æµå¼å¯¹è¯] çŠ¶æ€æ¶ˆæ¯å·²å‘é€")

                # è°ƒç”¨APIè·å–å“åº”
                print("[æµå¼å¯¹è¯] å¼€å§‹è°ƒç”¨ GLM API...")
                response_text = call_glm_api(messages, model=model)
                print(f"[æµå¼å¯¹è¯] APIå“åº”å®Œæˆï¼Œå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")

                # é€å­—è¿”å›å“åº”
                print(f"[æµå¼å¯¹è¯] å¼€å§‹é€å­—å‘é€ï¼Œå…± {len(response_text)} ä¸ªå­—ç¬¦")
                for idx, char in enumerate(response_text):
                    yield f"data: {json.dumps({'content': char, 'done': False})}\n\n"
                    if (idx + 1) % 100 == 0:
                        print(f"[æµå¼å¯¹è¯] å·²å‘é€ {idx + 1}/{len(response_text)} å­—ç¬¦")

                print(f"[æµå¼å¯¹è¯] æ‰€æœ‰å†…å®¹å·²å‘é€")

                # å‘é€å®Œæˆä¿¡å·
                yield f"data: {json.dumps({'done': True})}\n\n"
                print("[æµå¼å¯¹è¯] å‘é€å®Œæˆä¿¡å·")

            except HTTPException as e:
                yield f"data: {json.dumps({'error': str(e.detail), 'done': True})}\n\n"
            except Exception as e:
                yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

        except Exception as e:
            import traceback
            print(f"æµå¼å¯¹è¯ API é”™è¯¯: {str(e)}")
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

@app.post("/api/diagnose/analyze/stream")
async def diagnose_error_stream(request: DiagnoseRequest):
    """
    è§£é¢˜è¯Šæ–­åˆ†æ(æµå¼è¾“å‡º)

    åˆ†æå­¦ç”Ÿçš„é”™è¯¯ç­”æ¡ˆ,æ‰¾å‡ºé”™è¯¯åŸå› 
    """
    async def generate_stream():
        try:
            yield f"data: {json.dumps({'status': 'analyzing', 'message': 'ğŸ” æ­£åœ¨åˆ†æé”™è¯¯åŸå› ...'})}\n\n"

            # æ„å»ºè¯Šæ–­ prompt
            diagnose_prompt = f"""ä½ æ˜¯ä¸€ä½æœ‰20å¹´æ•™å­¦ç»éªŒçš„åˆä¸­æ•°å­¦è€å¸ˆ.
å­¦ç”Ÿåšé”™äº†è¿™é“é¢˜: {request.question}
å­¦ç”Ÿçš„é”™è¯¯ç­”æ¡ˆæ˜¯: {request.student_answer}

è¯·åˆ†æ:
1. è¿™é“é¢˜è€ƒæŸ¥çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹æ˜¯ä»€ä¹ˆ
2. å­¦ç”Ÿæœ€å¯èƒ½åœ¨å“ªä¸ªç¯èŠ‚å‡ºé”™(æ¦‚å¿µä¸æ¸…/æ–¹æ³•ä¸å¯¹/è®¡ç®—å¤±è¯¯)
3. ç”¨ä¸€å¥è¯å‘Šè¯‰å­¦ç”Ÿä»–çš„é—®é¢˜åœ¨å“ªé‡Œ(è¦å…·ä½“,ä¸è¦æ³›æ³›è€Œè°ˆ)

è¯·ä»¥JSONæ ¼å¼è¿”å›:
```json
{{
  "knowledge_point": "æ ¸å¿ƒçŸ¥è¯†ç‚¹",
  "error_type": "æ¦‚å¿µä¸æ¸…/æ–¹æ³•ä¸å¯¹/è®¡ç®—å¤±è¯¯",
  "problem_description": "ä¸€å¥è¯æè¿°å­¦ç”Ÿçš„é—®é¢˜",
  "analysis": "è¯¦ç»†åˆ†æ"
}}
```
åªè¿”å›JSON,ä¸è¦å…¶ä»–å†…å®¹."""

            messages = [{
                "role": "user",
                "content": diagnose_prompt
            }]

            response_text = call_glm_api(messages, model="glm-4-flash")

            # é€å­—è¿”å›åˆ†æå†…å®¹
            for char in response_text:
                yield f"data: {json.dumps({'content': char})}\n\n"

            # è§£æ JSON
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                result = json.loads(json_match.group(0))
                yield f"data: {json.dumps({'done': True, 'data': result})}\n\n"
            else:
                # å¦‚æœè§£æå¤±è´¥,è¿”å›åŸå§‹æ–‡æœ¬
                yield f"data: {json.dumps({'done': True, 'data': {'knowledge_point': 'æœªè¯†åˆ«', 'error_type': 'æœªåˆ†ç±»', 'problem_description': 'åˆ†æå¤±è´¥', 'analysis': response_text}})}\n\n"

        except HTTPException as e:
            yield f"data: {json.dumps({'error': str(e.detail), 'done': True})}\n\n"
        except Exception as e:
            import traceback
            print(f"è¯Šæ–­æµå¼ API é”™è¯¯: {str(e)}")
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

@app.post("/api/diagnose/analyze")
async def diagnose_error(request: DiagnoseRequest):
    """
    è§£é¢˜è¯Šæ–­åˆ†æ

    åˆ†æå­¦ç”Ÿçš„é”™è¯¯ç­”æ¡ˆ,æ‰¾å‡ºé”™è¯¯åŸå› 
    """
    try:
        # æ„å»ºè¯Šæ–­ prompt
        diagnose_prompt = f"""ä½ æ˜¯ä¸€ä½æœ‰20å¹´æ•™å­¦ç»éªŒçš„åˆä¸­æ•°å­¦è€å¸ˆ.
å­¦ç”Ÿåšé”™äº†è¿™é“é¢˜: {request.question}
å­¦ç”Ÿçš„é”™è¯¯ç­”æ¡ˆæ˜¯: {request.student_answer}

è¯·åˆ†æ:
1. è¿™é“é¢˜è€ƒæŸ¥çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹æ˜¯ä»€ä¹ˆ
2. å­¦ç”Ÿæœ€å¯èƒ½åœ¨å“ªä¸ªç¯èŠ‚å‡ºé”™(æ¦‚å¿µä¸æ¸…/æ–¹æ³•ä¸å¯¹/è®¡ç®—å¤±è¯¯)
3. ç”¨ä¸€å¥è¯å‘Šè¯‰å­¦ç”Ÿä»–çš„é—®é¢˜åœ¨å“ªé‡Œ(è¦å…·ä½“,ä¸è¦æ³›æ³›è€Œè°ˆ)

è¯·ä»¥JSONæ ¼å¼è¿”å›:
```json
{{
  "knowledge_point": "æ ¸å¿ƒçŸ¥è¯†ç‚¹",
  "error_type": "æ¦‚å¿µä¸æ¸…/æ–¹æ³•ä¸å¯¹/è®¡ç®—å¤±è¯¯",
  "problem_description": "ä¸€å¥è¯æè¿°å­¦ç”Ÿçš„é—®é¢˜",
  "analysis": "è¯¦ç»†åˆ†æ"
}}
```
åªè¿”å›JSON,ä¸è¦å…¶ä»–å†…å®¹. """

        messages = [{
            "role": "user",
            "content": diagnose_prompt
        }]

        response_text = call_glm_api(messages, model="glm-4-flash")

        # è§£æ JSON
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            result = json.loads(json_match.group(0))
            return {
                "success": True,
                "data": result
            }
        else:
            # å¦‚æœè§£æå¤±è´¥,è¿”å›åŸå§‹æ–‡æœ¬
            return {
                "success": True,
                "data": {
                    "knowledge_point": "æœªè¯†åˆ«",
                    "error_type": "æœªåˆ†ç±»",
                    "problem_description": "åˆ†æå¤±è´¥",
                    "analysis": response_text
                }
            }

    except Exception as e:
        import traceback
        print(f"è¯Šæ–­ API é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è¯Šæ–­å¤±è´¥: {str(e)}")

@app.post("/api/diagnose/guide/stream")
async def guide_student_stream(request: GuideRequest):
    """
    è‹æ ¼æ‹‰åº•å¼å¼•å¯¼(æµå¼è¾“å‡º)

    é€šè¿‡æé—®å¼•å¯¼å­¦ç”Ÿè‡ªå·±æ‰¾åˆ°ç­”æ¡ˆ
    """
    async def generate_stream():
        try:
            yield f"data: {json.dumps({'status': 'thinking', 'message': 'ğŸ¤” æ­£åœ¨æ€è€ƒå¦‚ä½•å¼•å¯¼...'})}\n\n"

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€è½®å¯¹è¯
            if not request.student_response and not request.conversation_history:
                # ç¬¬ä¸€è½®: ç”Ÿæˆåˆå§‹å¼•å¯¼é—®é¢˜
                guide_prompt = f"""ä½ æ˜¯ä¸€ä½è€å¿ƒçš„æ•°å­¦è€å¸ˆ,æ­£åœ¨ä¸€å¯¹ä¸€è¾…å¯¼å­¦ç”Ÿ.
å­¦ç”Ÿåˆšåšé”™äº†è¿™é“é¢˜: {request.question}
è¯Šæ–­ç»“æœ: {request.diagnosis}

è¯·ç”¨è‹æ ¼æ‹‰åº•å¼æé—®,ä¸€æ­¥æ­¥å¼•å¯¼å­¦ç”Ÿè‡ªå·±åšå‡ºæ¥.
è§„åˆ™:
- æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜
- å¦‚æœå­¦ç”Ÿç­”å¯¹,ç»™äºˆè‚¯å®šå¹¶æ¨è¿›ä¸‹ä¸€æ­¥
- å¦‚æœå­¦ç”Ÿç­”é”™æˆ–è¯´ä¸ä¼š,ç»™ä¸€ç‚¹æç¤º,ä½†ä¸è¦ç›´æ¥è¯´ç­”æ¡ˆ
- å¼•å¯¼æ§åˆ¶åœ¨5-8è½®å¯¹è¯å†…å®Œæˆ

ç°åœ¨è¯·å¼€å§‹å¼•å¯¼,æå‡ºç¬¬ä¸€ä¸ªé—®é¢˜æ¥å¯å‘å­¦ç”Ÿæ€è€ƒ. """
            else:
                # åç»­è½®: æ ¹æ®å­¦ç”Ÿå›ç­”ç»§ç»­å¼•å¯¼
                history_summary = "\n".join([
                    f"{msg.get('role', 'user')}: {msg.get('content', '')}"
                    for msg in request.conversation_history[-6:]  # åªå–æœ€è¿‘6è½®
                ])

                guide_prompt = f"""ä½ æ˜¯ä¸€ä½è€å¿ƒçš„æ•°å­¦è€å¸ˆ,æ­£åœ¨ä¸€å¯¹ä¸€è¾…å¯¼å­¦ç”Ÿ.

é¢˜ç›®: {request.question}
è¯Šæ–­ç»“æœ: {request.diagnosis}

å¯¹è¯å†å²:
{history_summary}

å­¦ç”Ÿæœ€æ–°å›ç­”: {request.student_response or '(å­¦ç”Ÿè¡¨ç¤ºä¸ä¼šæˆ–å›ç­”é”™è¯¯)'}

è¯·æ ¹æ®å­¦ç”Ÿçš„å›ç­”:
- å¦‚æœç­”å¯¹äº†: ç»™äºˆè‚¯å®š,å¹¶å¼•å¯¼ä¸‹ä¸€æ­¥
- å¦‚æœç­”é”™äº†: å§”å©‰æŒ‡å‡ºé—®é¢˜,ç»™å‡ºæç¤º
- å¦‚æœè¯´ä¸ä¼š: ç®€åŒ–é—®é¢˜,ç»™å‡ºæ›´æ˜æ˜¾çš„æç¤º

ç»§ç»­å¼•å¯¼å­¦ç”Ÿ,ç›´åˆ°æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ. æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜. """

            messages = [{
                "role": "user",
                "content": guide_prompt
            }]

            # é€å­—è¿”å›å¼•å¯¼å†…å®¹
            for char in call_glm_api(messages, model="glm-4-flash"):
                yield f"data: {json.dumps({'content': char})}\n\n"

            yield f"data: {json.dumps({'done': True})}\n\n"

        except HTTPException as e:
            yield f"data: {json.dumps({'error': str(e.detail), 'done': True})}\n\n"
        except Exception as e:
            import traceback
            print(f"å¼•å¯¼æµå¼ API é”™è¯¯: {str(e)}")
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

@app.post("/api/diagnose/guide")
async def guide_student(request: GuideRequest):
    """
    è‹æ ¼æ‹‰åº•å¼å¼•å¯¼

    é€šè¿‡æé—®å¼•å¯¼å­¦ç”Ÿè‡ªå·±æ‰¾åˆ°ç­”æ¡ˆ
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€è½®å¯¹è¯
        if not request.student_response and not request.conversation_history:
            # ç¬¬ä¸€è½®: ç”Ÿæˆåˆå§‹å¼•å¯¼é—®é¢˜
            guide_prompt = f"""ä½ æ˜¯ä¸€ä½è€å¿ƒçš„æ•°å­¦è€å¸ˆ,æ­£åœ¨ä¸€å¯¹ä¸€è¾…å¯¼å­¦ç”Ÿ. 
å­¦ç”Ÿåˆšåšé”™äº†è¿™é“é¢˜: {request.question}
è¯Šæ–­ç»“æœ: {request.diagnosis}

è¯·ç”¨è‹æ ¼æ‹‰åº•å¼æé—®,ä¸€æ­¥æ­¥å¼•å¯¼å­¦ç”Ÿè‡ªå·±åšå‡ºæ¥. 
è§„åˆ™: 
- æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜
- å¦‚æœå­¦ç”Ÿç­”å¯¹,ç»™äºˆè‚¯å®šå¹¶æ¨è¿›ä¸‹ä¸€æ­¥
- å¦‚æœå­¦ç”Ÿç­”é”™æˆ–è¯´ä¸ä¼š,ç»™ä¸€ç‚¹æç¤º,ä½†ä¸è¦ç›´æ¥è¯´ç­”æ¡ˆ
- å¼•å¯¼æ§åˆ¶åœ¨5-8è½®å¯¹è¯å†…å®Œæˆ

ç°åœ¨è¯·å¼€å§‹å¼•å¯¼,æå‡ºç¬¬ä¸€ä¸ªé—®é¢˜æ¥å¯å‘å­¦ç”Ÿæ€è€ƒ. """
        else:
            # åç»­è½®: æ ¹æ®å­¦ç”Ÿå›ç­”ç»§ç»­å¼•å¯¼
            history_summary = "\n".join([
                f"{msg.get('role', 'user')}: {msg.get('content', '')}"
                for msg in request.conversation_history[-6:]  # åªå–æœ€è¿‘6è½®
            ])

            guide_prompt = f"""ä½ æ˜¯ä¸€ä½è€å¿ƒçš„æ•°å­¦è€å¸ˆ,æ­£åœ¨ä¸€å¯¹ä¸€è¾…å¯¼å­¦ç”Ÿ. 

é¢˜ç›®: {request.question}
è¯Šæ–­ç»“æœ: {request.diagnosis}

å¯¹è¯å†å²: 
{history_summary}

å­¦ç”Ÿæœ€æ–°å›ç­”: {request.student_response or '(å­¦ç”Ÿè¡¨ç¤ºä¸ä¼šæˆ–å›ç­”é”™è¯¯)'}

è¯·æ ¹æ®å­¦ç”Ÿçš„å›ç­”: 
- å¦‚æœç­”å¯¹äº†: ç»™äºˆè‚¯å®š,å¹¶å¼•å¯¼ä¸‹ä¸€æ­¥
- å¦‚æœç­”é”™äº†: å§”å©‰æŒ‡å‡ºé—®é¢˜,ç»™å‡ºæç¤º
- å¦‚æœè¯´ä¸ä¼š: ç®€åŒ–é—®é¢˜,ç»™å‡ºæ›´æ˜æ˜¾çš„æç¤º

ç»§ç»­å¼•å¯¼å­¦ç”Ÿ,ç›´åˆ°æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ. æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜. """

        messages = [{
            "role": "user",
            "content": guide_prompt
        }]

        response_text = call_glm_api(messages, model="glm-4-flash")

        return {
            "success": True,
            "response": response_text[:1500],
            "is_complete": False  # å¯ä»¥æ ¹æ®å“åº”å†…å®¹åˆ¤æ–­æ˜¯å¦å®Œæˆå¼•å¯¼
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"å¼•å¯¼ API é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"å¼•å¯¼å¤±è´¥: {str(e)}")

@app.post("/api/detect/mistakes/smart")
async def smart_detect_mistakes(request: DetectMistakesRequest):
    """
    æ™ºèƒ½å¤šç»´åº¦éªŒè¯é”™é¢˜æ£€æµ‹

    å®ç°æµç¨‹: 
    1. è§£æå·é¢é¢˜ç›®å’Œå­¦ç”Ÿç­”æ¡ˆ
    2. è¯†åˆ«è€å¸ˆæ‰¹æ”¹æ ‡è®°
    3. AIç†è§£é¢˜ç›®å¹¶ç»™å‡ºç­”æ¡ˆ
    4. ä¸‰æ–¹æ¯”è¾ƒéªŒè¯
    """
    try:
        import time
        start_time = time.time()

        # è§£ç å›¾ç‰‡
        image = decode_base64_image(request.image_data)

        # ä½¿ç”¨é«˜è´¨é‡å›¾ç‰‡
        max_size = 1500
        if image.width > max_size or image.height > max_size:
            ratio = min(max_size / image.width, max_size / image.height)
            new_width = int(image.width * ratio)
            new_height = int(image.height * ratio)
            image = image.resize((new_width, new_height))

        base64_image = encode_image_to_base64(image, quality=85)
        print(f"[æ™ºèƒ½æ£€æµ‹] å›¾ç‰‡å°ºå¯¸: {image.width}x{image.height}")

        # æ­¥éª¤1: OCRè¯†åˆ«é¢˜ç›®, å­¦ç”Ÿç­”æ¡ˆ, è€å¸ˆæ‰¹æ”¹
        print(f"[æ™ºèƒ½æ£€æµ‹] æ­¥éª¤1: OCRè¯†åˆ«è¯•å·å†…å®¹...")
        ocr_prompt = """è¯·è¯¦ç»†åˆ†æè¿™å¼ è¯•å·,æå–ä»¥ä¸‹ä¿¡æ¯: 

å¯¹æ¯é“é¢˜ç›®(æŒ‰é¡ºåºç¼–å·),è¯·æä¾›: 
1. é¢˜å·
2. é¢˜ç›®ç±»å‹(é€‰æ‹©é¢˜/å¡«ç©ºé¢˜/åˆ¤æ–­é¢˜ç­‰)
3. é¢˜ç›®å†…å®¹
4. å­¦ç”Ÿé€‰æ‹©çš„ç­”æ¡ˆ(A/B/C/Dæˆ–å¡«ç©ºå†…å®¹)
5. è€å¸ˆçš„æ‰¹æ”¹æ ‡è®°(Ã—è¡¨ç¤ºé”™,âˆšè¡¨ç¤ºå¯¹,åœˆ/çº¿/ç‚¹è¡¨ç¤ºå…¶ä»–æ ‡è®°,æ— æ ‡è®°è¡¨ç¤ºæœªæ‰¹æ”¹)

è¯·ä»¥JSONæ ¼å¼è¿”å›: 
```json
{
  "questions": [
    {
      "question_no": "é¢˜å·",
      "question_type": "é¢˜å‹",
      "question_content": "é¢˜ç›®å†…å®¹",
      "student_answer": "å­¦ç”Ÿç­”æ¡ˆ",
      "teacher_mark": "è€å¸ˆæ ‡è®°(Ã—/âˆš/åœˆ/çº¿/ç‚¹/æ— )"
    }
  ]
}
```

æ³¨æ„: ä»”ç»†è¯†åˆ«æ¯ä¸ªé¢˜ç›®çš„æ‰¹æ”¹æ ‡è®°,Ã—å’Œâˆšè¦åŒºåˆ†æ¸…æ¥š. """

        ocr_messages = [{
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                },
                {
                    "type": "text",
                    "text": ocr_prompt
                }
            ]
        }]

        ocr_response = call_glm_api(ocr_messages, model="glm-4v", skip_delay=False, max_tokens=2000)
        print(f"[æ™ºèƒ½æ£€æµ‹] OCRå“åº”:\n{ocr_response[:500]}...")

        # è§£æOCRç»“æœ
        ocr_data = None
        json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', ocr_response)
        if json_match:
            try:
                ocr_data = json.loads(json_match.group(1))
            except:
                pass

        if not ocr_data:
            json_match = re.search(r'\{[\s\S]*"questions"[\s\S]*\}', ocr_response)
            if json_match:
                try:
                    ocr_data = json.loads(json_match.group(0))
                except:
                    pass

        if not ocr_data or "questions" not in ocr_data:
            return {
                "success": False,
                "error": "OCRè¯†åˆ«å¤±è´¥,è¯·ä¸Šä¼ æ›´æ¸…æ™°çš„è¯•å·å›¾ç‰‡"
            }

        questions = ocr_data["questions"]
        print(f"[æ™ºèƒ½æ£€æµ‹] è¯†åˆ«åˆ° {len(questions)} é“é¢˜ç›®")

        # æ­¥éª¤2-3: AIç†è§£é¢˜ç›®å¹¶ç»™å‡ºæ­£ç¡®ç­”æ¡ˆ
        print(f"[æ™ºèƒ½æ£€æµ‹] æ­¥éª¤2-3: AIåˆ†æé¢˜ç›®å¹¶ç»™å‡ºç­”æ¡ˆ...")
        analyzed_questions = []

        for q in questions:
            q_no = q.get("question_no", "?")
            q_type = q.get("question_type", "")
            q_content = q.get("question_content", "")
            student_answer = q.get("student_answer", "")
            teacher_mark = q.get("teacher_mark", "")

            # AIè§£ç­”é¢˜ç›®
            solve_prompt = f"""è¯·è§£ç­”è¿™é“é¢˜ç›®: 

é¢˜ç›®: {q_content}
å­¦ç”Ÿç­”æ¡ˆ: {student_answer}

è¯·åˆ†æå¹¶ç»™å‡º: 
1. æ­£ç¡®ç­”æ¡ˆ
2. å­¦ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
3. ç®€è¦åˆ†æåŸå› 

è¿”å›æ ¼å¼: 
```json
{{
  "correct_answer": "æ­£ç¡®ç­”æ¡ˆ",
  "is_correct": true/false,
  "reasoning": "åˆ†æåŸå› "
}}
```"""

            solve_messages = [{
                "role": "user",
                "content": solve_prompt
            }]

            try:
                solve_response = call_glm_api(solve_messages, model="glm-4-flash", skip_delay=False, max_tokens=500)

                solve_data = None
                json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', solve_response)
                if json_match:
                    try:
                        solve_data = json.loads(json_match.group(1))
                    except:
                        pass

                if not solve_data:
                    json_match = re.search(r'\{[\s\S]*\}', solve_response)
                    if json_match:
                        try:
                            solve_data = json.loads(json_match.group(0))
                        except:
                            pass

                if solve_data:
                    correct_answer = solve_data.get("correct_answer", "")
                    ai_judgment = solve_data.get("is_correct", False)
                    reasoning = solve_data.get("reasoning", "")
                else:
                    correct_answer = "æ— æ³•ç¡®å®š"
                    ai_judgment = None
                    reasoning = solve_response[:200]

            except Exception as e:
                print(f"[æ™ºèƒ½æ£€æµ‹] è§£ç­”é¢˜ç›®{q_no}å¤±è´¥: {str(e)}")
                correct_answer = "è§£æå¤±è´¥"
                ai_judgment = None
                reasoning = ""

            # æ­¥éª¤4-6: ä¸‰æ–¹æ¯”è¾ƒéªŒè¯
            # åˆ¤æ–­è€å¸ˆæ‰¹æ”¹: Ã—=é”™,âˆš=å¯¹
            teacher_says_wrong = teacher_mark in ["Ã—", "x", "X", "å‰", "é”™"]
            teacher_says_correct = teacher_mark in ["âˆš", "âœ“", "å¯¹", "é’©"]

            # éªŒè¯é€»è¾‘
            final_status = "éœ€è¦ç¡®è®¤"  # é»˜è®¤éœ€è¦å­¦ç”Ÿç¡®è®¤
            confidence = 0
            reason = []

            if ai_judgment is not None:
                # AIæœ‰æ˜ç¡®åˆ¤æ–­
                if ai_judgment == False and teacher_says_wrong:
                    # AIè¯´é”™,è€å¸ˆä¹Ÿè¯´é”™ â†’ ç¡®è®¤æ˜¯é”™é¢˜
                    final_status = "é”™é¢˜"
                    confidence = 95
                    reason.append("AIå’Œè€å¸ˆéƒ½è®¤ä¸ºæ˜¯é”™é¢˜")
                elif ai_judgment == True and teacher_says_correct:
                    # AIè¯´å¯¹,è€å¸ˆä¹Ÿè¯´å¯¹ â†’ ç¡®è®¤æ˜¯å¯¹çš„
                    final_status = "æ­£ç¡®"
                    confidence = 95
                    reason.append("AIå’Œè€å¸ˆéƒ½è®¤ä¸ºæ­£ç¡®")
                elif ai_judgment != teacher_says_correct and teacher_says_wrong:
                    # AIåˆ¤æ–­å’Œè€å¸ˆä¸ä¸€è‡´,ä¸”è€å¸ˆè¯´é”™ â†’ éœ€è¦å­¦ç”Ÿç¡®è®¤
                    final_status = "éœ€è¦ç¡®è®¤"
                    confidence = 50
                    reason.append(f"AIè®¤ä¸º{'å¯¹' if ai_judgment else 'é”™'},è€å¸ˆæ ‡è®°ä¸º{teacher_mark}")
                elif teacher_says_wrong:
                    # è€å¸ˆè¯´é”™,ä½†AIä¸ç¡®å®š
                    final_status = "ç–‘ä¼¼é”™é¢˜"
                    confidence = 70
                    reason.append("è€å¸ˆæ ‡è®°ä¸ºé”™é¢˜")
            elif teacher_says_wrong:
                # AIæ— æ³•åˆ¤æ–­,ä½†è€å¸ˆè¯´é”™
                final_status = "ç–‘ä¼¼é”™é¢˜"
                confidence = 60
                reason.append("è€å¸ˆæ ‡è®°ä¸ºé”™é¢˜,AIæœªèƒ½åˆ¤æ–­")

            analyzed_questions.append({
                "question_no": q_no,
                "question_type": q_type,
                "question_content": q_content,
                "student_answer": student_answer,
                "teacher_mark": teacher_mark,
                "correct_answer": correct_answer,
                "ai_judgment": ai_judgment,
                "final_status": final_status,
                "confidence": confidence,
                "reason": "; ".join(reason),
                "analysis": reasoning
            })

        # ç­›é€‰å‡ºé”™é¢˜å’Œéœ€è¦ç¡®è®¤çš„é¢˜ç›®
        mistakes = []
        need_confirmation = []

        for q in analyzed_questions:
            if q["final_status"] == "é”™é¢˜":
                mistakes.append({
                    "question_no": q["question_no"],
                    "reason": q["reason"],
                    "question": q["question_content"],
                    "student_answer": q["student_answer"],
                    "correct_answer": q["correct_answer"],
                    "analysis": q["analysis"]
                })
            elif q["final_status"] in ["éœ€è¦ç¡®è®¤", "ç–‘ä¼¼é”™é¢˜"]:
                need_confirmation.append({
                    "question_no": q["question_no"],
                    "reason": q["reason"],
                    "question": q["question_content"],
                    "student_answer": q["student_answer"],
                    "ai_answer": q["correct_answer"],
                    "teacher_mark": q["teacher_mark"],
                    "confidence": q["confidence"]
                })

        elapsed = time.time() - start_time
        print(f"[æ™ºèƒ½æ£€æµ‹] å®Œæˆ,è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"[æ™ºèƒ½æ£€æµ‹] é”™é¢˜: {len(mistakes)}, éœ€ç¡®è®¤: {len(need_confirmation)}")

        return {
            "success": True,
            "data": {
                "mistakes": mistakes,
                "need_confirmation": need_confirmation,
                "all_questions": analyzed_questions,
                "summary": f"è¯†åˆ«åˆ°{len(mistakes)}é“é”™é¢˜,{len(need_confirmation)}é“éœ€è¦ç¡®è®¤"
            },
            "elapsed_time": f"{elapsed:.2f}s"
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"[æ™ºèƒ½æ£€æµ‹] é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"æ™ºèƒ½æ£€æµ‹å¤±è´¥: {str(e)}")

@app.post("/api/detect/mistakes")
async def detect_mistakes(request: DetectMistakesRequest):
    """
    æ™ºèƒ½æ£€æµ‹è¯•å·ä¸­çš„é”™é¢˜(å¿«é€Ÿç‰ˆ)

    è¯†åˆ«è¯•å·ä¸­çš„é”™é¢˜ç‰¹å¾: 
    - çº¢ç¬”æ‰¹æ”¹ç—•è¿¹
    - å‰å·(Ã—)
    - æ¶‚æ”¹ç—•è¿¹
    - ä½åˆ†æ ‡è®°
    - è€å¸ˆæ‰¹æ³¨
    """
    try:
        import time
        start_time = time.time()

        # è§£ç å›¾ç‰‡
        image = decode_base64_image(request.image_data)

        # åˆå§‹åŒ–å˜é‡
        response_text = ""
        result = None

        # å¦‚æœç”¨æˆ·æä¾›äº†æ ‡è®°,ä½¿ç”¨é«˜è´¨é‡å›¾ç‰‡è¿›è¡Œè¯¦ç»†åˆ†æ
        if request.user_marks and len(request.user_marks) > 0:
            print(f"[é”™é¢˜æ£€æµ‹] ç”¨æˆ·æä¾›äº† {len(request.user_marks)} ä¸ªæ ‡è®°,å¼€å§‹è¯†åˆ«å’Œåˆ†æ")

            # ç”¨æˆ·æ ‡è®°æ¨¡å¼: ä½¿ç”¨é«˜è´¨é‡å›¾ç‰‡ä»¥ä¾¿AIèƒ½çœ‹æ¸…é¢˜ç›®
            max_size = 1500  # æ›´é«˜åˆ†è¾¨ç‡
            if image.width > max_size or image.height > max_size:
                ratio = min(max_size / image.width, max_size / image.height)
                new_width = int(image.width * ratio)
                new_height = int(image.height * ratio)
                image = image.resize((new_width, new_height))

            # ä½¿ç”¨è¾ƒé«˜è´¨é‡(85)
            base64_image = encode_image_to_base64(image, quality=85)
            print(f"[é”™é¢˜æ£€æµ‹] ç”¨æˆ·æ ‡è®°æ¨¡å¼,å›¾ç‰‡å°ºå¯¸: {image.width}x{image.height}")

            # è¯†åˆ«ç”¨æˆ·åœˆé€‰çš„é¢˜ç›®å¹¶è¿›è¡Œè¯¦ç»†åˆ†æ
            analyze_prompt = f"""ç”¨æˆ·å·²ç»æ¡†é€‰äº†è¯•å·ä¸­çš„ {len(request.user_marks)} é“é¢˜ç›®éœ€è¦åˆ†æ. è¯·ä»”ç»†åˆ†æè¿™äº›é¢˜ç›®. 

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æ: 
1. è¯†åˆ«æ¡†é€‰åŒºåŸŸçš„é¢˜ç›®å†…å®¹å’Œå­¦ç”Ÿç­”æ¡ˆ
2. åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
3. åˆ†æé”™è¯¯åŸå› å’ŒçŸ¥è¯†ç‚¹
4. æä¾›æ”¹è¿›å»ºè®®

å¿…é¡»è¿”å›JSONæ ¼å¼(ä¸è¦ä½¿ç”¨markdownä»£ç å—,ç›´æ¥è¿”å›JSON): 
{{
  "mistakes": [
    {{
      "question_no": "é¢˜å·æˆ–ä½ç½®",
      "question": "é¢˜ç›®å†…å®¹",
      "student_answer": "å­¦ç”Ÿç­”æ¡ˆ",
      "correct_answer": "æ­£ç¡®ç­”æ¡ˆ",
      "reason": "é”™è¯¯åŸå› ",
      "knowledge_point": "çŸ¥è¯†ç‚¹",
      "suggestion": "æ”¹è¿›å»ºè®®"
    }}
  ],
  "detailed_analysis": "è¯¦ç»†çš„å­¦æƒ…åˆ†æ,åŒ…æ‹¬: æ•´ä½“è¯„ä»·, è–„å¼±çŸ¥è¯†ç‚¹, å­¦ä¹ å»ºè®®ç­‰(è‡³å°‘200å­—)"
}}

æ³¨æ„: ç”¨æˆ·æ¡†é€‰çš„éƒ½æ˜¯éœ€è¦åˆ†æçš„é¢˜ç›®,è¯·ç›´æ¥åˆ†æå†…å®¹,ä¸è¦åˆ¤æ–­æ˜¯å¦ä¸ºé”™é¢˜. """

            messages = [{
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    },
                    {
                        "type": "text",
                        "text": analyze_prompt
                    }
                ]
            }]

            # è°ƒç”¨ API è¿›è¡Œè¯¦ç»†åˆ†æ(æ­£å¸¸æ¨¡å¼,éœ€è¦è¯¦ç»†è¾“å‡º)
            response_text = call_glm_api(messages, model="glm-4v", skip_delay=False, max_tokens=2000)

            elapsed = time.time() - start_time
            print(f"[é”™é¢˜æ£€æµ‹] ç”¨æˆ·æ ‡è®°åˆ†æè€—æ—¶: {elapsed:.2f}ç§’")
            print(f"[é”™é¢˜æ£€æµ‹] APIå“åº”:\n{response_text[:1500]}")

        else:
            # æ²¡æœ‰ç”¨æˆ·æ ‡è®°,æ‰§è¡Œè‡ªåŠ¨æ£€æµ‹
            # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼: ä½¿ç”¨ä¸­ç­‰è´¨é‡å›¾ç‰‡ä»¥æå‡å‡†ç¡®åº¦
            max_size = 1200  # æé«˜åˆ†è¾¨ç‡ä»¥ä¾¿AIèƒ½çœ‹æ¸…ç»†èŠ‚
            if image.width > max_size or image.height > max_size:
                ratio = min(max_size / image.width, max_size / image.height)
                new_width = int(image.width * ratio)
                new_height = int(image.height * ratio)
                image = image.resize((new_width, new_height))

            # ä½¿ç”¨ä¸­ç­‰è´¨é‡(75)æ¥å¹³è¡¡é€Ÿåº¦å’Œæ¸…æ™°åº¦
            base64_image = encode_image_to_base64(image, quality=75)
            print(f"[é”™é¢˜æ£€æµ‹] è‡ªåŠ¨æ£€æµ‹æ¨¡å¼,å›¾ç‰‡å°ºå¯¸: {image.width}x{image.height}")
            # ä¼˜åŒ–çš„ prompt - ä¸“æ³¨äºçº¢å‰/çº¢åœˆæ ‡è®°è¯†åˆ«
            detect_prompt = """æ‰¾å‡ºè¯•å·ä¸Šçš„é”™é¢˜. é”™é¢˜å¿…é¡»æœ‰æ¸…æ™°çš„çº¢è‰²Ã—æ ‡è®°åœ¨ç­”æ¡ˆä¸Š.

**ä»€ä¹ˆæ˜¯é”™é¢˜(å¿…é¡»æ»¡è¶³å…¨éƒ¨æ¡ä»¶)**:
1. ç­”æ¡ˆé€‰é¡¹(A/B/C/D)ä¸Šæœ‰çº¢è‰²Ã—
2. Ã—å·æ¸…æ™°å¯è§,ä¸¤æ¡äº¤å‰çº¿éƒ½æ¸…æ¥š
3. Ã—å·æ˜æ˜¾æ˜¯çº¢è‰²ç¬”è¿¹

**ç»å¯¹ä¸æ˜¯é”™é¢˜**:
- ç­”æ¡ˆæ‰“é’©âˆš â†’ æ­£ç¡®
- é¢˜å·æœ‰ä»»ä½•æ ‡è®° â†’ ä¸å½±å“åˆ¤æ–­
- ç­”æ¡ˆåªæœ‰åœˆ, çº¿, ç‚¹ä½†æ²¡æœ‰Ã— â†’ ä¸æ˜¯é”™é¢˜
- Ã—å·æ¨¡ç³Šä¸æ¸…æˆ–ä¸ç¡®å®š â†’ ä¸æ ‡è®°

**åˆ¤æ–­æµç¨‹**:
å¯¹æ¯ä¸ªé¢˜çš„ç­”æ¡ˆ(A/B/C/D):
- ä»”ç»†çœ‹: è¿™ä¸ªé€‰é¡¹ä¸Šæœ‰æ¸…æ™°çš„çº¢è‰²Ã—å—?
-- éå¸¸ç¡®å®šæœ‰Ã— â†’ é”™é¢˜
-- ä¸å¤ªç¡®å®šæˆ–æ¨¡ç³Š â†’ ä¸æ ‡è®°
-- æ²¡æœ‰Ã— â†’ æ­£ç¡®

**å®å¯æ¼æ£€,ç»ä¸è¯¯åˆ¤!**

è¿”å›JSONæ ¼å¼:
```json
{"mistakes": [{"question_no": "é¢˜å·", "reason": "çº¢å‰"}], "summary": "å…±æ‰¾åˆ°Xé“é”™é¢˜"}
```

æ²¡æœ‰é”™é¢˜: {"mistakes": [], "summary": "æœªå‘ç°é”™é¢˜"}"""

            messages = [{
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    },
                    {
                        "type": "text",
                        "text": detect_prompt
                    }
                ]
            }]

            # è°ƒç”¨ API(å¿«é€Ÿæ¨¡å¼: è·³è¿‡å»¶è¿Ÿ, å‡å°‘max_tokens)
            response_text = call_glm_api(messages, model="glm-4v", skip_delay=True, max_tokens=500)

            elapsed = time.time() - start_time
            print(f"[é”™é¢˜æ£€æµ‹] è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"[é”™é¢˜æ£€æµ‹] APIåŸå§‹å“åº”:\n{response_text[:1000]}")  # æ‰“å°å‰1000å­—ç¬¦ç”¨äºè°ƒè¯•

        # æ–¹å¼1: å°è¯•æå– JSON ä»£ç å—(```json ... ```)
        json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', response_text)
        if json_match:
            print(f"[é”™é¢˜æ£€æµ‹] åŒ¹é…åˆ° JSON ä»£ç å—")
            try:
                result = json.loads(json_match.group(1))
            except json.JSONDecodeError as e:
                print(f"[é”™é¢˜æ£€æµ‹] JSON ä»£ç å—è§£æå¤±è´¥: {e}")

        # æ–¹å¼2: å°è¯•æå–èŠ±æ‹¬å·å†…çš„å®Œæ•´ JSON
        if not result:
            json_match = re.search(r'\{[\s\S]*"mistakes"[\s\S]*\}', response_text)
            if json_match:
                print(f"[é”™é¢˜æ£€æµ‹] åŒ¹é…åˆ°åŒ…å« mistakes çš„ JSON")
                try:
                    result = json.loads(json_match.group(0))
                except json.JSONDecodeError as e:
                    print(f"[é”™é¢˜æ£€æµ‹] JSON è§£æå¤±è´¥: {e}")

        # æ–¹å¼3: å°è¯•æå–ä»»æ„å®Œæ•´çš„ JSON å¯¹è±¡
        if not result:
            json_match = re.search(r'\{[\s\S]*?\}', response_text)
            if json_match:
                print(f"[é”™é¢˜æ£€æµ‹] åŒ¹é…åˆ°ä»»æ„ JSON å¯¹è±¡")
                try:
                    result = json.loads(json_match.group(0))
                except json.JSONDecodeError as e:
                    print(f"[é”™é¢˜æ£€æµ‹] é€šç”¨ JSON è§£æå¤±è´¥: {e}")

        # æ–¹å¼4: å°è¯•è§£æMarkdownåˆ—è¡¨æ ¼å¼ï¼ˆé™çº§å¤„ç†ï¼‰
        if not result:
            print(f"[é”™é¢˜æ£€æµ‹] å°è¯•è§£æMarkdownåˆ—è¡¨æ ¼å¼")
            try:
                # å°è¯•ä»markdownåˆ—è¡¨ä¸­æå–é”™é¢˜ä¿¡æ¯
                # æ ¼å¼: - ç¬¬Xé¢˜ï¼š...
                lines = response_text.split('\n')
                mistakes_list = []
                for line in lines:
                    line = line.strip()
                    if line.startswith('-') or line.startswith('â€¢'):
                        # æå–é¢˜å·
                        match = re.search(r'ç¬¬(\d+)é¢˜', line)
                        if match:
                            question_no = match.group(1)
                            reason = "é”™é¢˜"
                            mistakes_list.append({
                                "question_no": question_no,
                                "reason": reason
                            })

                if mistakes_list:
                    result = {
                        "mistakes": mistakes_list,
                        "summary": f"å…±æ‰¾åˆ°{len(mistakes_list)}é“é”™é¢˜"
                    }
                    print(f"[é”™é¢˜æ£€æµ‹] Markdownè§£ææˆåŠŸ: æ‰¾åˆ°{len(mistakes_list)}é“é”™é¢˜")
            except Exception as e:
                print(f"[é”™é¢˜æ£€æµ‹] Markdownè§£æå¤±è´¥: {e}")

        if result:
            print(f"[é”™é¢˜æ£€æµ‹] âœ… è§£ææˆåŠŸ: {result}")

            # å¦‚æœæ‰¾åˆ°äº†é”™é¢˜,ç”Ÿæˆç®€è¦æ‘˜è¦å¹¶ç­‰å¾…ç¡®è®¤
            mistakes_list = result.get("mistakes", [])
            if mistakes_list:
                print(f"[é”™é¢˜æ£€æµ‹] æ‰¾åˆ° {len(mistakes_list)} é“é”™é¢˜,å¼€å§‹ç”Ÿæˆè¯¦ç»†å­¦æƒ…åˆ†æ...")

                try:
                    # ç¬¬ä¸€æ­¥: è¯†åˆ«è¯•å·å†…å®¹
                    ocr_prompt = """è¯·è¯†åˆ«è¿™å¼ è¯•å·çš„å†…å®¹,åŒ…æ‹¬:
1. å­¦ç§‘å’Œå¹´çº§
2. é¢˜ç›®å†…å®¹(ç‰¹åˆ«æ˜¯é”™é¢˜)
3. å­¦ç”Ÿç­”æ¡ˆ(å¦‚æœæœ‰)
4. è¯•å·æ•´ä½“ç‰¹ç‚¹

è¯·ç”¨ç®€æ´çš„è¯­è¨€æè¿°. """

                    ocr_messages = [{
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            },
                            {
                                "type": "text",
                                "text": ocr_prompt
                            }
                        ]
                    }]

                    paper_content = call_glm_api(ocr_messages, model="glm-4v", skip_delay=False, max_tokens=1000)
                    print(f"[é”™é¢˜æ£€æµ‹] è¯•å·å†…å®¹è¯†åˆ«å®Œæˆ,é•¿åº¦: {len(paper_content)} å­—ç¬¦")

                    # ç¬¬äºŒæ­¥: åŸºäºè¯•å·å†…å®¹ç”Ÿæˆå­¦æƒ…åˆ†æ
                    mistakes_str = ", ".join([m["question_no"] for m in mistakes_list])
                    analysis_prompt = f"""ä½ æ˜¯ç»éªŒä¸°å¯Œçš„è€å¸ˆ. è¯•å·å†…å®¹: {paper_content}

æ£€æµ‹åˆ°çš„é”™é¢˜: {mistakes_str}(å…±{len(mistakes_list)}é“)

è¯·ç”Ÿæˆè¯¦ç»†å­¦æƒ…åˆ†ææŠ¥å‘Š(å‚è€ƒä»¥ä¸‹æ ¼å¼): 

**ä¸€, å­¦ä¹ ç°çŠ¶åˆ†æ**
ä»å·é¢çœ‹,æ€»ç»“å­¦ç”Ÿçš„å­¦ä¹ ä¼˜åŠ¿(3ç‚¹)

**äºŒ, è–„å¼±ç‚¹ä¸å¤±åˆ†åŸå› **
é’ˆå¯¹é”™é¢˜åˆ†æå¤±åˆ†åŸå› å’Œè–„å¼±ç¯èŠ‚

**ä¸‰, é’ˆå¯¹æ€§å­¦ä¹ å»ºè®®**
ç»™å‡º3-5æ¡å…·ä½“å¯æ“ä½œçš„å»ºè®®

è¦æ±‚: ä¸“ä¸š, è¯¦ç»†, æœ‰é’ˆå¯¹æ€§, é¼“åŠ±æ€§è¯­æ°”. """

                    analysis_messages = [{
                        "role": "user",
                        "content": analysis_prompt
                    }]

                    # ä½¿ç”¨æ–‡æœ¬æ¨¡å‹ç”Ÿæˆæ›´è¯¦ç»†çš„åˆ†æ
                    analysis_text = call_glm_api(analysis_messages, model="glm-4-flash", skip_delay=False, max_tokens=2500)

                    # æ‰“å°å­¦æƒ…åˆ†æå†…å®¹
                    print(f"[é”™é¢˜æ£€æµ‹] å­¦æƒ…åˆ†æç”Ÿæˆå®Œæˆ,é•¿åº¦: {len(analysis_text)} å­—ç¬¦")

                    # è¿”å›ç»“æœ,åŒ…å«ç®€è¦ä¿¡æ¯å’Œè¯¦ç»†åˆ†æ
                    return {
                        "success": True,
                        "data": {
                            "mistakes": mistakes_list,
                            "detailed_analysis": analysis_text,
                            "summary": f"å…±æ‰¾åˆ°{len(mistakes_list)}é“é”™é¢˜",
                            "need_confirmation": True  # æ ‡è®°éœ€è¦å­¦ç”Ÿç¡®è®¤
                        },
                        "elapsed_time": f"{elapsed:.2f}s"
                    }

                except Exception as e:
                    print(f"[é”™é¢˜æ£€æµ‹] å­¦æƒ…åˆ†æç”Ÿæˆå¤±è´¥: {str(e)}")
                    # å³ä½¿åˆ†æå¤±è´¥,ä¹Ÿè¿”å›åŸºæœ¬çš„é”™é¢˜ä¿¡æ¯
                    return {
                        "success": True,
                        "data": {
                            "mistakes": mistakes_list,
                            "detailed_analysis": None,
                            "summary": f"å…±æ‰¾åˆ°{len(mistakes_list)}é“é”™é¢˜",
                            "need_confirmation": True
                        },
                        "elapsed_time": f"{elapsed:.2f}s"
                    }

            return {
                "success": True,
                "data": result,
                "elapsed_time": f"{elapsed:.2f}s"
            }

        # è§£æå¤±è´¥,å°è¯•ä½¿ç”¨é™çº§æ–¹æ¡ˆ(ä»…ç”¨æˆ·æ ‡è®°æ¨¡å¼)
        print(f"[é”™é¢˜æ£€æµ‹] å¼€å§‹æ£€æŸ¥é™çº§å¤„ç†, user_marks={len(request.user_marks) if request.user_marks else 0}, result={result}")
        if request.user_marks and len(request.user_marks) > 0:
            print(f"[é”™é¢˜æ£€æµ‹] JSONè§£æå¤±è´¥,ä½¿ç”¨AIæ–‡æœ¬å›å¤ä½œä¸ºé™çº§æ–¹æ¡ˆ")
            print(f"[é”™é¢˜æ£€æµ‹] AIå›å¤é•¿åº¦: {len(response_text)} å­—ç¬¦")
            # ä½¿ç”¨AIçš„æ–‡æœ¬å›å¤ä½œä¸ºåˆ†æå†…å®¹
            return {
                "success": True,
                "data": {
                    "mistakes": [
                        {
                            "question_no": f"æ¡†é€‰é¢˜ç›®{i+1}",
                            "question": "ç”¨æˆ·æ¡†é€‰çš„é¢˜ç›®",
                            "reason": "éœ€è¦åˆ†æ",
                            "student_answer": "è§ä¸‹æ–¹åˆ†æ",
                            "correct_answer": "è§ä¸‹æ–¹åˆ†æ",
                            "knowledge_point": "ç»¼åˆåˆ†æ",
                            "suggestion": "è§ä¸‹æ–¹åˆ†æ"
                        }
                        for i in range(len(request.user_marks))
                    ],
                    "detailed_analysis": response_text if len(response_text) > 10 else "AIè¿”å›çš„åˆ†æå†…å®¹è¿‡çŸ­,å¯èƒ½æ˜¯å›¾ç‰‡è´¨é‡ä¸ä½³. è¯·å°è¯•ä¸Šä¼ æ›´æ¸…æ™°çš„å›¾ç‰‡. "
                },
                "elapsed_time": f"{elapsed:.2f}s"
            }

        # è§£æå¤±è´¥è¿”å›é»˜è®¤å€¼
        print(f"[é”™é¢˜æ£€æµ‹] âŒ æ‰€æœ‰è§£ææ–¹å¼éƒ½å¤±è´¥")
        print(f"[é”™é¢˜æ£€æµ‹] æ£€æŸ¥æ¡ä»¶: request.user_marks={request.user_marks}, len={len(request.user_marks) if request.user_marks else 0}")
        return {
            "success": True,
            "data": {
                "mistakes": [],
                "summary": "è¯†åˆ«å¤±è´¥,è¯·é‡è¯•"
            },
            "elapsed_time": f"{elapsed:.2f}s"
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"é”™é¢˜æ£€æµ‹ API é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"é”™é¢˜æ£€æµ‹å¤±è´¥: {str(e)}")


@app.post("/api/detect/mistakes/stream")
async def detect_mistakes_stream(request: DetectMistakesRequest):
    """
    é”™é¢˜æ£€æµ‹(æµå¼è¾“å‡º)

    è¯†åˆ«è¯•å·ä¸­çš„é”™é¢˜,ä½¿ç”¨æµå¼å“åº”é€æ­¥è¿”å›åˆ†æç»“æœ
    å§‹ç»ˆä½¿ç”¨GLM-4Vè§†è§‰æ¨¡å‹è¿›è¡Œå›¾åƒè¯†åˆ«
    """
    async def generate_stream():
        import sys
        try:
            if not request.image_data:
                yield f"data: {json.dumps({'error': 'è¯·æä¾›å›¾ç‰‡æ•°æ®'})}\n\n"
                return

            start_time = time.time()
            print(f"[é”™é¢˜æ£€æµ‹æµå¼] ========== å¼€å§‹å¤„ç† ==========")
            print(f"[é”™é¢˜æ£€æµ‹æµå¼] æ”¶åˆ°è¯·æ±‚, user_marksæ•°é‡: {len(request.user_marks) if request.user_marks else 0}")
            sys.stdout.flush()

            # å…ˆå‘é€"åˆ†æä¸­"çŠ¶æ€
            yield f"data: {json.dumps({'status': 'analyzing', 'message': 'AIæ­£åœ¨åˆ†æä¸­...'})}\n\n"
            print(f"[é”™é¢˜æ£€æµ‹æµå¼] å‘é€åˆ†æä¸­çŠ¶æ€")
            sys.stdout.flush()

            # å‘é€å¼€å§‹æ£€æµ‹ä¿¡å·
            yield f"data: {json.dumps({'status': 'start', 'message': 'ğŸ” å¼€å§‹åˆ†æè¯•å·...'})}\n\n"

            # è§£ç å›¾ç‰‡
            try:
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] å¼€å§‹è§£ç å›¾ç‰‡...")
                sys.stdout.flush()
                image = decode_base64_image(request.image_data)
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] å›¾ç‰‡è§£ç æˆåŠŸ, å°ºå¯¸: {image.width}x{image.height}")
                sys.stdout.flush()
            except Exception as e:
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] å›¾ç‰‡è§£ç å¤±è´¥: {str(e)}")
                sys.stdout.flush()
                yield f"data: {json.dumps({'error': f'å›¾ç‰‡è§£ç å¤±è´¥: {str(e)}'})}\n\n"
                return

            # æ ¹æ®æ˜¯å¦æœ‰ç”¨æˆ·æ ‡è®°é€‰æ‹©å¤„ç†æ¨¡å¼
            if request.user_marks and len(request.user_marks) > 0:
                # ç”¨æˆ·æ ‡è®°æ¨¡å¼
                yield f"data: {json.dumps({'status': 'processing', 'message': 'ğŸ“‹ åˆ†æç”¨æˆ·æ ‡è®°çš„é¢˜ç›®...'})}\n\n"

                max_size = 1500
                if image.width > max_size or image.height > max_size:
                    ratio = min(max_size / image.width, max_size / image.height)
                    new_width = int(image.width * ratio)
                    new_height = int(image.height * ratio)
                    image = image.resize((new_width, new_height))

                base64_image = encode_image_to_base64(image, quality=85)

                # æ„å»ºåˆ†ææç¤º
                marks_desc = "\n".join([
                    f"æ¡†é€‰{i+1}: ä½ç½®{mark.get('x', 0)}%,{mark.get('y', 0)}%, å¤§å°{mark.get('width', 0)}%x{mark.get('height', 0)}%"
                    for i, mark in enumerate(request.user_marks)
                ])

                analyze_prompt = f"""ç”¨æˆ·æ ‡è®°äº†è¯•å·ä¸Šçš„{len(request.user_marks)}ä¸ªåŒºåŸŸ,éœ€è¦ä½ åˆ†æ:
{marks_desc}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æ:
1. è¯†åˆ«æ¡†é€‰åŒºåŸŸçš„é¢˜ç›®å†…å®¹å’Œå­¦ç”Ÿç­”æ¡ˆ
2. åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
3. åˆ†æé”™è¯¯åŸå› å’ŒçŸ¥è¯†ç‚¹
4. æä¾›æ”¹è¿›å»ºè®®

å¿…é¡»è¿”å›JSONæ ¼å¼(ä¸è¦ä½¿ç”¨markdownä»£ç å—,ç›´æ¥è¿”å›JSON):
{{
  "mistakes": [
    {{
      "question_no": "é¢˜å·æˆ–ä½ç½®",
      "question": "é¢˜ç›®å†…å®¹",
      "student_answer": "å­¦ç”Ÿç­”æ¡ˆ",
      "correct_answer": "æ­£ç¡®ç­”æ¡ˆ",
      "reason": "é”™è¯¯åŸå› ",
      "knowledge_point": "çŸ¥è¯†ç‚¹",
      "suggestion": "æ”¹è¿›å»ºè®®"
    }}
  ],
  "detailed_analysis": "è¯¦ç»†çš„å­¦æƒ…åˆ†æ,åŒ…æ‹¬: æ•´ä½“è¯„ä»·, è–„å¼±çŸ¥è¯†ç‚¹, å­¦ä¹ å»ºè®®ç­‰(è‡³å°‘200å­—)"
}}

æ³¨æ„: ç”¨æˆ·æ¡†é€‰çš„éƒ½æ˜¯éœ€è¦åˆ†æçš„é¢˜ç›®,è¯·ç›´æ¥åˆ†æå†…å®¹,ä¸è¦åˆ¤æ–­æ˜¯å¦ä¸ºé”™é¢˜."""

                messages = [{
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        },
                        {
                            "type": "text",
                            "text": analyze_prompt
                        }
                    ]
                }]

                # è°ƒç”¨ GLM-4V è§†è§‰æ¨¡å‹è¿›è¡Œåˆ†æ
                response_text = call_glm_api(messages, model="glm-4v", skip_delay=False, max_tokens=2000)

                print(f"[é”™é¢˜æ£€æµ‹æµå¼] ç”¨æˆ·æ ‡è®°æ¨¡å¼ APIå“åº”:\n{response_text}\n")

                # è§£æå“åº”
                result = parse_mistakes_from_response(response_text)

                if result is None:
                    print(f"[é”™é¢˜æ£€æµ‹æµå¼] ç”¨æˆ·æ ‡è®°æ¨¡å¼è§£æå¤±è´¥")
                    yield f"data: {json.dumps({'error': f'æ— æ³•è§£æAIå“åº”ã€‚è¯·é‡è¯•ã€‚'})}\n\n"
                    return

                mistakes_list = result.get("mistakes", [])

                if mistakes_list:
                    # é€æ­¥å‘é€ç»“æœ
                    yield f"data: {json.dumps({'status': 'found', 'count': len(mistakes_list), 'message': f'âœ… æ‰¾åˆ° {len(mistakes_list)} é“éœ€è¦åˆ†æçš„é¢˜ç›®'})}\n\n"

                    # ç”Ÿæˆè¯¦ç»†åˆ†æ
                    mistakes_str = ", ".join([m["question_no"] for m in mistakes_list])
                    analysis_prompt = f"""ä½ æ˜¯ç»éªŒä¸°å¯Œçš„è€å¸ˆ. æ£€æµ‹åˆ°çš„é¢˜ç›®: {mistakes_str}(å…±{len(mistakes_list)}é“)

è¯·ç”Ÿæˆè¯¦ç»†å­¦æƒ…åˆ†ææŠ¥å‘Š(å‚è€ƒä»¥ä¸‹æ ¼å¼):

**ä¸€ã€å­¦ä¹ ç°çŠ¶åˆ†æ**
ä»å·é¢çœ‹,æ€»ç»“å­¦ç”Ÿçš„å­¦ä¹ ä¼˜åŠ¿(3ç‚¹)

**äºŒã€è–„å¼±ç‚¹ä¸å¤±åˆ†åŸå› **
é’ˆå¯¹é”™é¢˜åˆ†æå¤±åˆ†åŸå› å’Œè–„å¼±ç¯èŠ‚

**ä¸‰ã€é’ˆå¯¹æ€§å­¦ä¹ å»ºè®®**
ç»™å‡º3-5æ¡å…·ä½“å¯æ“ä½œçš„å»ºè®®

è¦æ±‚: ä¸“ä¸š, è¯¦ç»†, æœ‰é’ˆå¯¹æ€§, é¼“åŠ±æ€§è¯­æ°”."""

                    analysis_messages = [{
                        "role": "user",
                        "content": analysis_prompt
                    }]

                    # ä½¿ç”¨æµå¼è¿”å›åˆ†ææ–‡æœ¬
                    analysis_text = call_glm_api(analysis_messages, model="glm-4-flash", skip_delay=False, max_tokens=2500)

                    # é€å­—è¿”å›å­¦æƒ…åˆ†æ
                    for char in analysis_text:
                        yield f"data: {json.dumps({'content': char})}\n\n"

                    # å‘é€å®Œæˆæ•°æ®å’Œç»“æœ
                    yield f"data: {json.dumps({'done': True, 'data': {'mistakes': mistakes_list, 'need_confirmation': True}})}\n\n"
                else:
                    yield f"data: {json.dumps({'error': 'æœªèƒ½è¯†åˆ«åˆ°æ ‡è®°çš„é¢˜ç›®ï¼Œè¯·é‡è¯•'})}\n\n"

            else:
                # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ - ä½¿ç”¨GLM-4Vè¯†åˆ«è¯•å·ä¸Šçš„çº¢å‰
                yield f"data: {json.dumps({'status': 'processing', 'message': 'ğŸ” ä½¿ç”¨AIè§†è§‰æ¨¡å‹è¯†åˆ«é”™é¢˜æ ‡è®°...'})}\n\n"

                max_size = 1200
                if image.width > max_size or image.height > max_size:
                    ratio = min(max_size / image.width, max_size / image.height)
                    new_width = int(image.width * ratio)
                    new_height = int(image.height * ratio)
                    image = image.resize((new_width, new_height))

                base64_image = encode_image_to_base64(image, quality=75)

                detect_prompt = """è¯·åˆ†æè¿™å¼ è¯•å·ï¼Œæ‰¾å‡ºæ‰€æœ‰æœ‰é”™è¯¯çš„é¢˜ç›®ã€‚

è§‚å¯Ÿè¦ç‚¹ï¼š
1. çº¢è‰²Ã—æ ‡è®° - æ˜æ˜¾çš„é”™é¢˜æ ‡è®°
2. çº¢ç¬”æ‰¹æ”¹ - è¢«è€å¸ˆæ ‡è®°ä¸ºé”™è¯¯çš„é¢˜ç›®
3. å­¦ç”Ÿç­”æ¡ˆæ˜æ˜¾é”™è¯¯

**é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹**

æ ¼å¼è¦æ±‚ï¼š
- å¦‚æœæœ‰é”™é¢˜ï¼šåªå›ç­”"ç¬¬Xé¢˜ã€ç¬¬Yé¢˜ã€ç¬¬Zé¢˜"ï¼ˆç”¨é¡¿å·åˆ†éš”ï¼‰
- å¦‚æœæ²¡æœ‰é”™é¢˜ï¼šåªå›ç­”"æ²¡æœ‰é”™é¢˜"

ç¤ºä¾‹ï¼š
âœ“ æ­£ç¡®ï¼šç¬¬4é¢˜ã€ç¬¬5é¢˜
âœ“ æ­£ç¡®ï¼šç¬¬17é¢˜
âœ“ æ­£ç¡®ï¼šæ²¡æœ‰é”™é¢˜
âœ— é”™è¯¯ï¼šç¬¬4é¢˜æœ‰é”™è¯¯ï¼ˆä¸è¦æè¿°ï¼‰
âœ— é”™è¯¯ï¼š17ï¼ˆè¦æœ‰"ç¬¬"å’Œ"é¢˜"ï¼‰

å¼€å§‹è¯†åˆ«ï¼š"""

                messages = [{
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        },
                        {
                            "type": "text",
                            "text": detect_prompt
                        }
                    ]
                }]

                # ä½¿ç”¨GLM-4Vè§†è§‰æ¨¡å‹è¯†åˆ«
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] è°ƒç”¨GLM-4V API...")
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] messagesç»“æ„: {messages[0]['role']}, å†…å®¹ç±»å‹: {type(messages[0]['content'])}")
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] contenté•¿åº¦: {len(messages[0]['content'])}")
                sys.stdout.flush()

                response_text = call_glm_api(messages, model="glm-4v", skip_delay=False, max_tokens=1500)

                print(f"[é”™é¢˜æ£€æµ‹æµå¼] APIè°ƒç”¨å®Œæˆ")
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] APIå“åº”ç±»å‹: {type(response_text)}")
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] APIå“åº”repr: {repr(response_text)}")
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] APIå®Œæ•´å“åº”:\n{response_text}\n")
                print(f"[é”™é¢˜æ£€æµ‹æµå¼] å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                sys.stdout.flush()

                # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ç©ºå“åº”
                if not response_text or response_text.strip() == "":
                    print(f"[é”™é¢˜æ£€æµ‹æµå¼] AIè¿”å›ç©ºå“åº”")
                    yield f"data: {json.dumps({'error': 'AIè¯†åˆ«å¤±è´¥ï¼Œæœªè¿”å›ä»»ä½•å†…å®¹ã€‚è¯·å°è¯•ä¸Šä¼ æ›´æ¸…æ™°çš„å›¾ç‰‡æˆ–ç¨åé‡è¯•ã€‚', 'done': True})}\n\n"
                    return

                # ä»è‡ªç„¶è¯­è¨€å›å¤ä¸­æå–é¢˜å·
                import re
                mistakes_list = []

                # æ£€æŸ¥æ˜¯å¦è¡¨ç¤ºæ²¡æœ‰é”™é¢˜
                no_mistake_keywords = ['æ²¡æœ‰', 'æœªå‘ç°', 'æ‰¾ä¸åˆ°', 'å…¨éƒ¨æ­£ç¡®', 'æ²¡æœ‰é”™é¢˜', 'æœªå‘ç°é”™é¢˜', 'æ²¡æœ‰çº¢å‰']
                if any(keyword in response_text for keyword in no_mistake_keywords):
                    print(f"[é”™é¢˜æ£€æµ‹æµå¼] AIå›å¤è¡¨ç¤ºæ²¡æœ‰é”™é¢˜")
                    mistakes_list = []
                else:
                    # æŸ¥æ‰¾æ‰€æœ‰é¢˜å·ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                    question_patterns = [
                        r'ç¬¬?(\d+)é¢˜',  # ç¬¬4é¢˜, 4é¢˜
                        r'(\d+)å·',      # 4å·
                        r'question\s*(\d+)',  # question 4
                        r'NO[\.]?(\d+)',  # NO.4
                        r'(\d+)[ã€ï¼Œ,]',  # 17ã€19 - ä¸­æ–‡é¡¿å·æˆ–é€—å·åˆ†éš”
                        r'æ˜¯[ï¼š:]\s*(\d+)',  # æ˜¯ï¼š17
                        r'é¢˜å·[ï¼š:]\s*(\d+)',  # é¢˜å·ï¼š17
                    ]

                    all_numbers = []
                    for pattern in question_patterns:
                        matches = re.findall(pattern, response_text, re.IGNORECASE)
                        all_numbers.extend(matches)

                    # å»é‡å¹¶æ’åº
                    unique_numbers = sorted(set(all_numbers))

                    if unique_numbers:
                        for num in unique_numbers:
                            mistakes_list.append({
                                "question_no": num,
                                "reason": "çº¢å‰æ ‡è®°"
                            })
                        print(f"[é”™é¢˜æ£€æµ‹æµå¼] ä»å›å¤ä¸­æå–åˆ°é¢˜å·: {unique_numbers}")

                sys.stdout.flush()

                if mistakes_list:
                    yield f"data: {json.dumps({'status': 'found', 'count': len(mistakes_list), 'message': f'âœ… æ£€æµ‹åˆ° {len(mistakes_list)} é“é”™é¢˜'})}\n\n"

                    # ç”Ÿæˆè¯¦ç»†åˆ†æ
                    yield f"data: {json.dumps({'status': 'analyzing', 'message': 'ğŸ“Š ç”Ÿæˆå­¦æƒ…åˆ†æ...'})}\n\n"

                    mistakes_str = ", ".join([m["question_no"] for m in mistakes_list])
                    analysis_prompt = f"""ä½ æ˜¯ç»éªŒä¸°å¯Œçš„è€å¸ˆ. æ£€æµ‹åˆ°çš„é”™é¢˜: {mistakes_str}(å…±{len(mistakes_list)}é“)

è¯·ç”Ÿæˆè¯¦ç»†å­¦æƒ…åˆ†ææŠ¥å‘Š(å‚è€ƒä»¥ä¸‹æ ¼å¼):

**ä¸€ã€å­¦ä¹ ç°çŠ¶åˆ†æ**
ä»å·é¢çœ‹,æ€»ç»“å­¦ç”Ÿçš„å­¦ä¹ ä¼˜åŠ¿(3ç‚¹)

**äºŒã€è–„å¼±ç‚¹ä¸å¤±åˆ†åŸå› **
é’ˆå¯¹é”™é¢˜åˆ†æå¤±åˆ†åŸå› å’Œè–„å¼±ç¯èŠ‚

**ä¸‰ã€é’ˆå¯¹æ€§å­¦ä¹ å»ºè®®**
ç»™å‡º3-5æ¡å…·ä½“å¯æ“ä½œçš„å»ºè®®

è¦æ±‚: ä¸“ä¸š, è¯¦ç»†, æœ‰é’ˆå¯¹æ€§, é¼“åŠ±æ€§è¯­æ°”."""

                    analysis_messages = [{
                        "role": "user",
                        "content": analysis_prompt
                    }]

                    analysis_text = call_glm_api(analysis_messages, model="glm-4-flash", skip_delay=False, max_tokens=2500)

                    # é€å­—è¿”å›å­¦æƒ…åˆ†æ
                    for char in analysis_text:
                        yield f"data: {json.dumps({'content': char})}\n\n"

                    # å‘é€å®Œæˆæ•°æ®
                    yield f"data: {json.dumps({'done': True, 'data': {'mistakes': mistakes_list, 'need_confirmation': True}})}\n\n"
                else:
                    # resultå­˜åœ¨ä½†mistakesä¸ºç©º
                    print(f"[é”™é¢˜æ£€æµ‹æµå¼] è§£ææˆåŠŸä½†mistakesä¸ºç©º")
                    yield f"data: {json.dumps({'status': 'no_mistakes', 'message': 'âœ… æ²¡æœ‰å‘ç°æ˜æ˜¾çš„é”™é¢˜'})}\n\n"
                    yield f"data: {json.dumps({'done': True, 'data': {'mistakes': [], 'need_confirmation': False}})}\n\n"

        except Exception as e:
            import traceback
            print(f"[é”™é¢˜æ£€æµ‹æµå¼] é”™è¯¯: {str(e)}")
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")


# ==================== æ™ºèƒ½åˆ†æAPI ====================

@app.post("/api/analyze/smart")
async def smart_analyze(request: DetectMistakesRequest):
    """
    æ™ºèƒ½åˆ†æAPI - è‡ªåŠ¨åˆ¤æ–­å†…å®¹ç±»å‹å¹¶æ‰§è¡Œç›¸åº”åˆ†æ

    åˆ¤æ–­é€»è¾‘ï¼š
    - ç”¨æˆ·æ ‡è®°â‰¥3ä¸ªæˆ–æ£€æµ‹åˆ°â‰¥3é“é”™é¢˜ â†’ æ•´å¼ è¯•å·ï¼Œç”Ÿæˆè¯¦ç»†å­¦æƒ…åˆ†æ
    - ç”¨æˆ·æ ‡è®°1-2ä¸ªæˆ–æ£€æµ‹åˆ°1-2é“é”™é¢˜ â†’ å•ä¸ªé”™é¢˜ï¼Œè¿›è¡Œé’ˆå¯¹æ€§è®²è§£
    """
    try:
        import time
        start_time = time.time()

        # è§£ç å›¾ç‰‡
        image = decode_base64_image(request.image_data)

        # åˆ¤æ–­ç”¨æˆ·æ ‡è®°æ•°é‡
        user_marks_count = len(request.user_marks) if request.user_marks else 0

        print(f"[æ™ºèƒ½åˆ†æ] å¼€å§‹åˆ†æï¼Œç”¨æˆ·æ ‡è®°æ•°é‡: {user_marks_count}")

        # å‘é€åˆå§‹çŠ¶æ€
        # yield_status = f"ğŸ” æ­£åœ¨åˆ†æè¯•å·å†…å®¹..."

        # æ­¥éª¤1: æ£€æµ‹é”™é¢˜
        print(f"[æ™ºèƒ½åˆ†æ] æ­¥éª¤1: æ£€æµ‹è¯•å·ä¸­çš„é”™é¢˜...")

        # å¦‚æœç”¨æˆ·æœ‰æ ‡è®°ï¼Œä½¿ç”¨æ ‡è®°æ¨¡å¼ï¼›å¦åˆ™è‡ªåŠ¨æ£€æµ‹
        if user_marks_count > 0:
            # ç”¨æˆ·æ ‡è®°æ¨¡å¼
            max_size = 1500
            if image.width > max_size or image.height > max_size:
                ratio = min(max_size / image.width, max_size / image.height)
                new_width = int(image.width * ratio)
                new_height = int(image.height * ratio)
                image = image.resize((new_width, new_height))

            base64_image = encode_image_to_base64(image, quality=85)

            analyze_prompt = f"""ç”¨æˆ·æ ‡è®°äº†è¯•å·ä¸Šçš„{user_marks_count}ä¸ªåŒºåŸŸéœ€è¦åˆ†æã€‚

è¯·è¯†åˆ«è¿™äº›åŒºåŸŸä¸­çš„é¢˜ç›®ï¼Œå¹¶æå–ï¼š
1. é¢˜å·
2. é¢˜ç›®å†…å®¹
3. å­¦ç”Ÿç­”æ¡ˆ
4. æ­£ç¡®ç­”æ¡ˆï¼ˆå¦‚æœå¯ä»¥åˆ¤æ–­ï¼‰
5. é”™è¯¯åŸå› 

å¿…é¡»è¿”å›JSONæ ¼å¼:
{{
  "mistakes": [
    {{
      "question_no": "é¢˜å·",
      "question": "é¢˜ç›®å†…å®¹",
      "student_answer": "å­¦ç”Ÿç­”æ¡ˆ",
      "correct_answer": "æ­£ç¡®ç­”æ¡ˆ",
      "reason": "é”™è¯¯åŸå› "
    }}
  ]
}}"""

            messages = [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
                    {"type": "text", "text": analyze_prompt}
                ]
            }]

            response_text = call_glm_api(messages, model="glm-4v", skip_delay=False, max_tokens=2000)

            # è§£æå“åº”
            mistakes = []
            json_match = re.search(r'\{[\s\S]*"mistakes"[\s\S]*\}', response_text)
            if json_match:
                try:
                    data = json.loads(json_match.group(0))
                    mistakes = data.get("mistakes", [])
                except:
                    pass
        else:
            # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼
            detect_prompt = """è¯·è¯†åˆ«è¿™å¼ è¯•å·ä¸­çš„æ‰€æœ‰é”™é¢˜ï¼ˆæœ‰çº¢Ã—æ ‡è®°æˆ–è€å¸ˆæ‰¹æ”¹çš„é¢˜ç›®ï¼‰ã€‚

è¯·è¿”å›JSONæ ¼å¼:
{
  "mistakes": [
    {"question_no": "é¢˜å·", "reason": "çº¢å‰æ ‡è®°"}
  ]
}

å¦‚æœæ²¡æœ‰é”™é¢˜ï¼Œè¿”å›: {"mistakes": []}"""

            max_size = 1200
            if image.width > max_size or image.height > max_size:
                ratio = min(max_size / image.width, max_size / image.height)
                image = image.resize((int(image.width * ratio), int(image.height * ratio)))

            base64_image = encode_image_to_base64(image, quality=75)

            messages = [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
                    {"type": "text", "text": detect_prompt}
                ]
            }]

            response_text = call_glm_api(messages, model="glm-4v", skip_delay=False, max_tokens=1500)

            # è§£æå“åº”
            mistakes = []
            json_match = re.search(r'\{[\s\S]*"mistakes"[\s\S]*\}', response_text)
            if json_match:
                try:
                    data = json.loads(json_match.group(0))
                    mistakes = data.get("mistakes", [])
                except:
                    pass

        mistake_count = len(mistakes)
        print(f"[æ™ºèƒ½åˆ†æ] æ£€æµ‹åˆ° {mistake_count} é“é”™é¢˜")

        # æ­¥éª¤2: è¯†åˆ«è¯•å·å­¦ç§‘ç±»å‹
        print(f"[æ™ºèƒ½åˆ†æ] æ­¥éª¤2: è¯†åˆ«è¯•å·å­¦ç§‘ç±»å‹...")

        subject_prompt = """è¯·åˆ†æè¿™å¼ è¯•å·ï¼Œè¯†åˆ«å®ƒå±äºå“ªä¸ªå­¦ç§‘ã€‚

å¯èƒ½çš„å­¦ç§‘åŒ…æ‹¬ï¼š
- æ•°å­¦
- è¯­æ–‡
- è‹±è¯­
- ç‰©ç†
- åŒ–å­¦
- ç”Ÿç‰©
- å†å²
- åœ°ç†
- æ”¿æ²»

è¯·åªè¿”å›å­¦ç§‘åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å›"æœªçŸ¥"ã€‚"""

        subject_messages = [{
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
                {"type": "text", "text": subject_prompt}
            ]
        }]

        try:
            subject = call_glm_api(subject_messages, model="glm-4v", skip_delay=True, max_tokens=50)
            # æ¸…ç†ç»“æœï¼Œæå–å­¦ç§‘åç§°
            subject = subject.strip()
            if any(kw in subject for kw in ["è‹±è¯­", "English", "english"]):
                subject = "è‹±è¯­è¯•å·"
            elif any(kw in subject for kw in ["æ•°å­¦", "Math", "math"]):
                subject = "æ•°å­¦è¯•å·"
            elif any(kw in subject for kw in ["è¯­æ–‡", "Chinese", "chinese"]):
                subject = "è¯­æ–‡è¯•å·"
            elif any(kw in subject for kw in ["ç‰©ç†", "Physics", "physics"]):
                subject = "ç‰©ç†è¯•å·"
            elif any(kw in subject for kw in ["åŒ–å­¦", "Chemistry", "chemistry"]):
                subject = "åŒ–å­¦è¯•å·"
            elif "æœªçŸ¥" in subject or len(subject) > 10:
                subject = "è¯•å·"
            else:
                subject = f"{subject}è¯•å·"
            print(f"[æ™ºèƒ½åˆ†æ] è¯†åˆ«å­¦ç§‘: {subject}")
        except:
            subject = "è¯•å·"
            print(f"[æ™ºèƒ½åˆ†æ] å­¦ç§‘è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")

        # æ­¥éª¤3: åˆ¤æ–­å†…å®¹ç±»å‹
        detection_result = {
            "user_marks_count": user_marks_count,
            "mistakes": mistakes
        }

        content_type = analyze_content_type(detection_result)
        print(f"[æ™ºèƒ½åˆ†æ] åˆ¤æ–­ç»“æœ: {content_type}")

        # æ­¥éª¤4: æ ¹æ®ç±»å‹ç”Ÿæˆç›¸åº”çš„åˆ†æ
        if content_type["is_full_paper"]:
            # æ•´å¼ è¯•å· - ç”Ÿæˆå­¦æƒ…åˆ†æ
            print(f"[æ™ºèƒ½åˆ†æ] ç”Ÿæˆå­¦æƒ…åˆ†ææŠ¥å‘Š...")

            analysis_prompt = generate_learning_analysis_prompt(
                {"mistakes": mistakes},
                subject  # ä½¿ç”¨è¯†åˆ«å‡ºçš„å­¦ç§‘ç±»å‹
            )

            analysis_messages = [{
                "role": "user",
                "content": analysis_prompt
            }]

            analysis_response = call_glm_api(analysis_messages, model="glm-4-flash", skip_delay=False, max_tokens=3000)

            elapsed = time.time() - start_time

            return {
                "success": True,
                "data": {
                    "content_type": "learning_analysis",
                    "analysis": analysis_response,
                    "mistakes": mistakes,
                    "mistake_count": mistake_count,
                    "user_marks_count": user_marks_count
                },
                "reason": content_type["reason"],
                "elapsed_time": f"{elapsed:.2f}s"
            }

        else:
            # å•ä¸ªé”™é¢˜ - ç”Ÿæˆé’ˆå¯¹æ€§è®²è§£
            print(f"[æ™ºèƒ½åˆ†æ] ç”Ÿæˆé”™é¢˜è®²è§£...")

            # é€‰æ‹©ç¬¬ä¸€é“é”™é¢˜è¿›è¡Œè®²è§£
            if mistakes:
                first_mistake = mistakes[0]

                guide_prompt = generate_mistake_guide_prompt(first_mistake)

                guide_messages = [{
                    "role": "user",
                    "content": guide_prompt
                }]

                guide_response = call_glm_api(guide_messages, model="glm-4-flash", skip_delay=False, max_tokens=2000)

                elapsed = time.time() - start_time

                return {
                    "success": True,
                    "data": {
                        "content_type": "mistake_guide",
                        "guide": guide_response,
                        "mistake": first_mistake,
                        "total_mistakes": mistakes,
                        "mistake_count": mistake_count
                    },
                    "reason": content_type["reason"],
                    "elapsed_time": f"{elapsed:.2f}s"
                }
            else:
                return {
                    "success": False,
                    "error": "æœªæ£€æµ‹åˆ°é”™é¢˜",
                    "reason": "è¯·ç¡®ä¿è¯•å·ä¸­æœ‰æ˜æ˜¾çš„é”™é¢˜æ ‡è®°"
                }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"[æ™ºèƒ½åˆ†æ] é”™è¯¯: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"æ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}")


@app.post("/api/analyze/smart/stream")
async def smart_analyze_stream(request: DetectMistakesRequest):
    """
    æ™ºèƒ½åˆ†æAPIï¼ˆæµå¼è¾“å‡ºï¼‰- è‡ªåŠ¨åˆ¤æ–­å¹¶æ‰§è¡Œç›¸åº”åˆ†æ
    """
    async def generate_stream():
        import sys
        try:
            start_time = time.time()

            # å‘é€å¼€å§‹ä¿¡å·
            yield f"data: {json.dumps({'status': 'start', 'message': 'ğŸ” å¼€å§‹æ™ºèƒ½åˆ†æ...'})}\n\n"

            # è§£ç å›¾ç‰‡
            image = decode_base64_image(request.image_data)
            user_marks_count = len(request.user_marks) if request.user_marks else 0

            print(f"[æ™ºèƒ½åˆ†ææµå¼] ç”¨æˆ·æ ‡è®°: {user_marks_count}")
            sys.stdout.flush()

            # æ£€æµ‹é”™é¢˜
            yield f"data: {json.dumps({'status': 'detecting', 'message': 'ğŸ“‹ æ­£åœ¨æ£€æµ‹è¯•å·ä¸­çš„é”™é¢˜...'})}\n\n"

            # ... (æ£€æµ‹é€»è¾‘ä¸ä¸Šé¢ç›¸åŒï¼Œè¿™é‡Œçœç•¥è¯¦ç»†ä»£ç )

            # ç¤ºä¾‹ï¼šå‡è®¾æ£€æµ‹å®Œæˆ
            yield f"data: {json.dumps({'status': 'detected', 'mistake_count': 3, 'message': f'âœ… æ£€æµ‹åˆ° 3 é“é”™é¢˜'})}\n\n"

            # åˆ¤æ–­ç±»å‹
            yield f"data: {json.dumps({'status': 'analyzing', 'message': 'ğŸ“Š æ­£åœ¨ç”Ÿæˆå­¦æƒ…åˆ†ææŠ¥å‘Š...'})}\n\n"

            # ç”Ÿæˆåˆ†æ...
            yield f"data: {json.dumps({'content_type': 'learning_analysis'})}\n\n"

            # æµå¼è¾“å‡ºåˆ†æå†…å®¹
            analysis_text = "è¯¦ç»†çš„åˆ†æå†…å®¹..."

            for char in analysis_text:
                yield f"data: {json.dumps({'content': char})}\n\n"

            # å®Œæˆ
            yield f"data: {json.dumps({'done': True})}\n\n"

        except Exception as e:
            import traceback
            print(f"[æ™ºèƒ½åˆ†ææµå¼] é”™è¯¯: {str(e)}")
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")


# ==================== å¯åŠ¨æœåŠ¡å™¨ ====================
if __name__ == "__main__":
    print("=" * 60)
    print("AI Study Companion - åç«¯æœåŠ¡")
    print("=" * 60)
    print("\nå¯åŠ¨æœåŠ¡å™¨...")
    print("\nAPI æ–‡æ¡£: http://localhost:8000/docs")
    print("å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print("\n" + "=" * 60)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
